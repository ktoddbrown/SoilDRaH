---
title: "Data Rescue: Ares 2001"
format:
  html:
    toc: true
date: last-modified
date-format: YYYY-MMMM
bibliography:
  - Ares2001_Methods.bib
  - Ares2001.bib
authors:
  - name:
      given: Katherine
      family: Todd-Brown
    orcid: 0000-0002-3109-8130
    note: https://github.com/ktoddbrown
    affiliation:
      - ref: uf-ees
    role: 
      - Transcription
      - Review
      - Standardization
      - Harmonization

  - name:
      given: Colby
      family: Green
    affiliation:
      - ref: uf-ees
    role: Transcription
    note: https://github.com/colbyGreen1520

  - name:
      given: Savannah
      family: Scott
    affiliation:
      - ref: uf-ees
    role: Transcription
    note: https://github.com/SavaScott
      
affiliations:
  - id: uf-ees
    name: University of Florida
    department: Environmental Engineering Sciences
    city: Gainesville
    state: FL
    country: USA
    url: https://essie.ufl.edu/ees/
---

```{r setup, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE}

library(tidyverse)
library(kableExtra)
library(bibtex)

#semantic files
of_variable.file <- '../../semantics/SoilDRaHVocabulary_of_variable.csv'
is_type.file <- '../../semantics/SoilDRaHVocabulary_is_type.csv'

# data files
methods.file <- 'Ares2001_Methods.md'
table1.file <- 'Ares2001_Table1.csv'
table3.file <- 'Ares2001_Table3.csv'
primaryCitation.file <- 'Ares2001.bib'
methodsCitation.file <- 'Ares2001_Methods.bib'

# read function
source('../../../R/readAres2001.R')
```

This documents the data transcription for @Ares2001 .
For the discussion of this data rescue see Github [issue 77](https://github.com/ktoddbrown/SoilDRaH/issues/77) .

> A. Ares and J. H. Fownes. Productivity, resource use, and competitive interactions of fraxinus uhdei in hawaii uplands. Canadian Journal of Forest Research, 31(1):132-142, [https://doi.org/10.1139/x00-156](https://doi.org/10.1139/x00-156). 2001 

# Data Summary

@Ares2001 primarily measures tree growth parameters (including litterfall) but also characterized elemental fractions of soil. They use this data to look at factors affecting a specific invasive tree growth in Hawaii including nitrogen and moisture gradients.

## Fit for purpose: HiCSC

This data is identified as a data source for the [Hawai'i Soil Organic Carbon Database](https://osf.io/hmtv6/) as part of the HiCSC.

- Location: Location region is identified and lat/lon can likely be recovered from figure 1 by regional expert.
- Soil carbon stock: Soil carbon fraction is given but not bulk density nor coarse fraction.
- Soil type: Soil order and history are identified in methods. High/low activity clay status may need inference by regional expert.
- Land use: Land use and history are clearly identified in methods.

# Tables and figures

- Table 
  1) characteristics of stands including oc percent < -- oc == organic carbon -->
  2) regression coefficients linking diameter at breast height to biomass and leaf area
  3) climate
  4) elevation vs productivity metrics
  5) fertilizer treatment vs productivity over time
  6) fertilizer vs leaf area index
  7) gradient vs plant efficiency metrics

- Figure 
  1) stand map overlayed on soil
  2) elevation vs productivity metrics
  3) nitrogen correlations
  4) light intercept vs nitrogen and water use efficiency metrics
  
### Table 1: Stand characteristics

Table 1 was modified to remove the sub-tables and flatted sub-tables in certain columns.

```{r table1}
#| code-fold: true
#| message: false

read_csv(file = table1.file,
                   skip = 1,
                   col_types = cols(.default = col_character())) |>
  kable(caption = read_csv(file = table1.file, 
                                 n_max = 1, col_names = 'caption', 
                                 show_col_types = FALSE)$caption)
```

### Table 3: Climate

```{r table3}
#| code-fold: true
#| message: false

read_csv(file = table3.file,
                   skip = 1,
                   col_types = cols(.default = col_character())) |>
  kable(caption = read_csv(file = table3.file, 
                                 n_max = 1, col_names = 'caption', 
                                 show_col_types = FALSE)$caption)
```


{{< include Ares2001_Methods.md >}}

<!--  # Transcription comparison
what purpose does this serve? this file does not exist in this file group -BW
This is here mostly to remind transcribers to cross check using diff the two transcripts. Let's keep this here but comment it out. -KTB
```{bash eval=FALSE}
diff Ares2001_Methods_CAG.md Ares2001_Methods.md
```
 -->
 
# Files

These files are in the Ares2001 data rescue.

- [Readme](README_Ares2001.qmd)
  + This is the primary file that documents the transcriptions and decision made during data rescue.
- [Ares2001.bib](Ares2001.bib)
  + citation for article transcribed
- [Ares2001_Methods.bib](Ares2001_Methods.bib)
  + citations for the methods section of the article
- [Ares2001_Methods.Rmd](Ares2001_Methods.Rmd)
  + methods transcribed from primary article
- [Ares2001_Table1.csv](Ares2001_Table1.csv)
  + table 1 from primary article with site descriptions
- [Ares2001_Table3.csv](Ares2001_Table3.csv)
  + table 3 from primary article with climate variables

<!--- comment out these working notes but preserve them as work done -KTB

# Working notes (KTB)

## Data Rescue Plan

- [x] describe files in human readable form inside ReadMe
- [x] description with Git issue with link
- [x] add contribution yaml
- [x] create excel sheet with Table 1 and 3
- [x] export excel table to csv
- [x] copy over methods section into markdown 
  - started, PDF doesn't copy but there is an html version which is incomplete and error ridden. Ended up with alot of retyping.
- [x] pull down citations in methods section to bib file
- [x] add in citation links to methods
- [x] cross check with second transcriber for tables and methods
- [ ] submit to independent review
- [ ] archive on SoilDRaH


## Citation notes from Methods

Below are the citation notes from the methods section.
Some citations are missing, those are noted here and how best guess were generated.

- [x] Sato et al. 1973
  + Sato, H.H., Ikeda, W., Paeth, R., Smythe, R., and Takehiro, M. 1973. Soil survey of the island of Hawaii, state of Hawaii. USDA Soil Conservation Service, University of Hawaii Agricultural Experiment Station, U.S. Government Printing Office, Washington D.C.
  + manually entered 
- [x] Harrington and Fownes (1993)
  + Harrington, R.A., and Fownes, J.H. 1993. Allometry and growth of planted versus coppice stands of four fast-growing tropical tree species. For. Ecol. Manage. 56: 315-327.
  + direct export
- [x] Sprugel 1983
  + Sprugel, D.G. 1983. Correcting for bias in log-transformed allometric equations. Ecology, 64: 209-210.
  + direct export
- [x] Avery 1975
  + Avery, T.E. 1975. Natural resources measurements. McGraw-Hill Inc., New York.
  + Manual add as a @book
- [x] Welles and Norman 1991
  + Welles, J.M., and Norman, J.M. 1991. Instrument for indirect measurement of canopy architecture. Agron. J. 83: 818-825.
  + direct export
- [x] Nelson and Sommers ~1972~ 1973
  + Nelson, D.W., and Sommers, L.E. 1972. Determination of total nitrogen in plant material. Agron. J. 65: 109-112.
  + correction to the year, 1973 instead
- [x] Isaac and Johnson 1983
  + Isaac, R.A., and Johnson, W.C. 1983. High speed analysis of agriculture samples using inductively coupled plasma-atomic emission spectroscopy. Spectrochim. Acta, 38B: 277-282.
  + direct export
- [x] Wolf 1974
  + Wolf, B. 1974. Improvements in the azomethine-H-method for the determination of boron. Comm. Soil Sci. Plant Anal. 5: 39-44.
  + direct export
- [x] Vitousek and Sanford 1986
  + Vitousek, PM., and Sanford, R.L. 1986. Nutrient cycling in moist tropical forests. Annu. Rev. Ecol. Syst. 17: 137-167.
  + direct export
  + correct number citation
- [x] Heanes 1984
  + Heanes, D. L. 1984. Determination of total organic C in soils by improved chromic acid digestion and spectrophotometric procedure. Commun. Soil Sci. Plant Anal. 15: 1191-1213.
  + direct export
--->

# Level 0 data read

This level 0 data read parses the transcribed files with minimal cleaning or transformations.

```{r readLevel0}
#This chunk has two purposes. 
#...1) Check the formatting by reading in everything 
#...2) Create a list of everything to process later in level 1

data.lvl0.ls <- list(
  #Read in a list of all the bib files
  citation = list(
    #Citation for the article transcriptions are pulled from
    primary = read.bib(file = primaryCitation.file), 
    #Citations for all referenced articles
    methods = read.bib(file = methodsCitation.file)
  ),
  #Read in the text transcription of the article's methods section
  method = read_lines(file = methods.file),
  #Read in the results as tables or figure transcriptions. This includes
  #...the caption as well as the tables themselves
  data = list(
    Table1 = list(
      #Read the caption as a text string. Captions are the first cell on 
      #...the first row.
      caption = read_csv(file = table1.file,
                         col_types = cols(.default = col_character()),
                         n_max = 1, col_names = FALSE)$X1[1],
      #Read in all the data, skipping the first row with the caption and read
      #...in the table as character. This element is a tibble (data.frame).
      primary = read_csv(file = table1.file,
                         col_types = cols(.default = col_character()),
                         skip = 1)
    ), 
    #Same format as Table1
    Table3 = list(
      caption = read_csv(file = table3.file,
                         col_types = cols(.default = col_character()),
                         n_max = 1, col_names = FALSE)$X1[1],
      primary = read_csv(file = table3.file,
                         col_types = cols(.default = col_character()),
                         skip = 1)
    )))

```

```{r}
#| label: checkReadFunction_lvl0

datafunction.lvl0.ls <- readAres2001(dataDir = '.',
                                    dataLevel = 'level0')

if(isTRUE(all.equal(datafunction.lvl0.ls, data.lvl0.ls))){
  print('Read function matches code here.')
}else{
  print('There is a mismatch in the data objects between the code here and the read function.')
}

```

# Level 1 data - Hawaii SOC DB

This data is identified as a data source for the [Hawai'i Soil Organic Carbon Database](https://osf.io/hmtv6/) as part of the HiCSC.

- Location: Location region is identified and lat/lon can likely be recovered from figure 1 by regional expert.
- Soil carbon stock: Soil carbon fraction is given but not bulk density nor coarse fraction.
- Soil type: Soil order and history are identified in methods. High/low activity clay status may need inference by regional expert.
- Land use: Land use and history are clearly identified in methods.

Information from the methods and other sections are pulled into tables with line by line source citations.
This information can be values, units, methods, or other useful notes and is manually extracted from the transcriptions loaded into the level 0 data above.

The final format of the data frame here will include the following 

  + grouping ids
  + `of_variable`
  + `is_type`
  + `with_entry`
  + `source`

The `of_variable` being used here are:

 + citation
 + diameter_at_breast_height_A_koa
 + diameter_at_breast_height_F_uhdei
 + doi
 + elevation
 + height_A_koa
 + height_F_uhdei
 + inital_planting
 + land_use_type
 + mean_air_temperature
 + mean_max_vapor_pressure_deficit
 + mean_solar_radiation
 + observation_year
 + region
 + soil_class
 + soil_nitrogen
 + soil_organic_carbon
 + soil_ph
 + soil_phosphorus
 + stand_age
 + stand_type
 + stem_density_A_koa
 + stem_density_F_uhdei
 + total_rainfall

```{r}
#| lable: constructLevel1

#Create table with data that is consistent across the study
studyMeta <- tribble(~of_variable, ~is_type, ~with_entry, ~source,
                   'region', 'value', 'Honaunau Forest on the southwestern slopes of Mauna Loa, island of Hawaii', paste('Method ln5:', paste(data.lvl0.ls$method[5], collapse = ' ')), #no actual lat-long, need to get this translated to geo-location
                   'land_use_type', 'value', 'Tree stands', paste('Method ln14-16:', paste(data.lvl0.ls$method[14:16], collapse = ' ')),
                   'inital_planting', 'value', '1959', paste('Method ln14:', data.lvl0.ls$method[14]),
                   'observation_year', 'value', '1996', paste('Method ln 30;53;58:', paste0(data.lvl0.ls$method[c(30,53,58)], collapse = '... ')),
                   'citation', 'value', format(data.lvl0.ls$citation$primary), 'journal citation',
                   'doi', 'value', data.lvl0.ls$citation$primary$doi, 'journal citation')

# Process the primary data from Table 1 by maintaining data groups and standardizing the variable name so that we can link this to the study metadata and data in the other tables when needed.
Table1Primary <- data.lvl0.ls$data$Table1$primary |>
  #Add a row id preserving row grouping -- not unique
  mutate(row_id = paste0('R', 1:n())) |>
  #Transform the table to long data by row group and column names. This allows us to then create the cross links between the tables.
  pivot_longer(cols = -row_id,
               names_to = 'column_name', values_to = 'with_entry',
               values_drop_na = TRUE) |> #drop missing values 
  #Link the variable names to the column names using a switch statement
  mutate(of_variable = case_when(
           column_name == "Stand type" ~ 'stand_type',
           column_name == 'Elevation (m)' ~ 'elevation',
           column_name == 'Soil type' ~ 'soil_class',
           column_name == 'Soil pH' ~ 'soil_ph',
           column_name == 'Soil organic carbon (%)' ~ 'soil_organic_carbon',
           column_name == 'Soil N (%)' ~ 'soil_nitrogen',
           column_name == 'Soil P (mg kg<sup>-1</sup>)' ~ 'soil_phosphorus',
           column_name == 'Stand age (years)' ~ 'stand_age',
           column_name == '*F. uhdei* Stem density (trees / ha)' ~ 'stem_density_F_uhdei',
           column_name == '*F. uhdei* Mean DBH (cm)' ~ 'diameter_at_breast_height_F_uhdei',
           column_name == '*F. uhdei* Mean height(m)' ~ 'height_F_uhdei',
           column_name == '*A. koa* Stem density (trees / ha)' ~ 'stem_density_A_koa',
           column_name == '*A. koa* Mean DBH (cm)' ~ 'diameter_at_breast_height_A_koa',
           column_name == "*A. koa* Mean height(m)"~ 'height_A_koa')) |>
  #Flag the source of this data as Table 1, this table is merged later with other data so this keeps track of where the data came from
  mutate(source = 'Table 1')

# Create the metadata for table one from the header information and cited methods
Table1Meta <- Table1Primary |>
  select(column_name, of_variable) |>
  unique() |>
  #Grab everything between the parentheses as units and attribute the source as the column names.
  mutate(unit = str_extract(column_name, pattern = '(?<=\\().*(?=\\))'),
         source = 'Table 1 column names.') |>
  #If there aren't units then drop the row
  filter(!is.na(unit)) |>
  #Don't keep the column name now that you have the units, cross link via the variable
  select(-column_name) |>
  #Stack into the table the methods for each variable that we can find
  bind_rows(
  tribble(~of_variable, ~method, ~source,
            'soil_ph', paste0(data.lvl0.ls$method[72:73], collapse = ' '), 'Methods ln72-73', #pasting in a method from specific rows
            'soil_organic_carbon', paste0(data.lvl0.ls$method[72:73], collapse = ' '), 'Methods ln72-73',
            'soil_nitrogen', paste0(data.lvl0.ls$method[72:73], collapse = ' '), 'Methods ln72-73',
            'soil_phosphorus', paste0(data.lvl0.ls$method[72:73], collapse = ' '), 'Methods ln72-73',
            'soil_class', paste0(data.lvl0.ls$method[10:12], collapse = ' '), 'Methods ln10-12') ) |>
  #Stack onto the tables the controlled vocabulary used
  bind_rows(
    tribble(~of_variable, ~control_vocabulary, ~source,
            'stand_type', '*F. uhdei*: pure stands of Fraxinus uhdei (Wenzig) Lingelsh|Mixed: mixed stands of *Fraxinus uhdei* (Wenzig) Lingelsh and *Acacia koa* Grey', 'Abstract ln1',
            'soil_class', 'Histosol:USDA classification for histosol soil type|Andisols:USDA classification for andisol soil type', 'expert informed')
  ) |>
  # Push the dimensions of the variable into a single column. Making this a long table.
  pivot_longer(cols = c(unit, method, control_vocabulary),
               names_to = 'is_type',
               values_drop_na = TRUE,
               values_to = 'with_entry')


#The following is doing the same process as above but fitted to the data and formatting of Table3
Table3Primary <- data.lvl0.ls$data$Table3$primary |>
  pivot_longer(cols = -variable,
               names_to = 'row_id', values_to = 'with_entry',
               values_drop_na = TRUE) |>
  #mutate(`elevation_id` = str_extract(elevation_id, '\\d{3,} m')) |>
  pivot_wider(names_from = 'variable', values_from = 'with_entry') |>
  mutate(`Elevation (m)` = str_extract(row_id, '\\d{3,}')) |>
  pivot_longer(cols = -row_id, 
               names_to = 'column_name', values_to = 'with_entry') |>
  mutate(of_variable = case_when(
    column_name == "Mean air temperature (degree C)"~ 'mean_air_temperature',
    column_name == "Total rainfall (mm)" ~ 'total_rainfall',
    column_name == "Mean total solar radiation (MJ m<sup>-2</sup> day<sup>-1</sup>)" ~ 'mean_solar_radiation',
    column_name == "Mean maximum vapor pressure deficit (kPa)" ~ 'mean_max_vapor_pressure_deficit',
    column_name == "Elevation (m)" ~ 'elevation',
    TRUE ~ NA_character_)) |>
  mutate(source = 'Table 3')

Table3Meta <- Table3Primary |>
  select(column_name, of_variable) |>
  unique() |>
  mutate(unit = str_extract(column_name, pattern = '(?<=\\().*(?=\\))'),
         method = paste('Methods ln72-73:', paste0(data.lvl0.ls$method[18:23], collapse = ' '))) |>
  select(-column_name) |>
  pivot_longer(cols = -of_variable,
               names_to = 'is_type', values_to = 'with_entry') |>
  mutate(source = case_when(is_type == 'unit' ~ 'Table 3 column names.',
                            is_type == 'method' ~ 'Methods ln72-73',
                            TRUE ~ NA_character_))


# Pull everything together into by stacking the meta and primary data tables
data.lvl1.ls <- list(
  meta = bind_rows(studyMeta, Table1Meta, Table3Meta),
  primary = bind_rows(Table1Primary, Table3Primary)|>
    mutate(elevation_id = with_entry[of_variable == 'elevation'],
           is_type = 'value',
           .by = row_id) |>
    arrange(row_id, elevation_id, column_name,
            of_variable, is_type, with_entry, source)
  )
#View(data.lvl1.ls)

```

## Check for new variables

`of_variable` in the data files should be defined in the SoilDRaH Vocabulary files.

```{r}
#| label: check_of_variable

SoilDRaH_variable_list <- read_csv(file =of_variable.file,
                                   show_col_types = FALSE)

if(all(unique(c(data.lvl1.ls$meta$of_variable,
            data.lvl1.ls$primary$of_variable)) %in% SoilDRaH_variable_list$of_variable)){
  print('All of_variables are in the SoilDRaH variable list. No additions required')
}else{
  print(paste0('Please start a issue for new variables [', setdiff(unique(c(data.lvl1.ls$meta$of_variable,
            data.lvl1.ls$primary$of_variable)), SoilDRaH_variable_list$of_variable), ']'))
}
```

## Check for is_type

`is_type` in the data files should be defined in the SoilDRaH Vocabulary files.

```{r}
#| label: check_is_type

SoilDRaH_type_list <- read_csv(file = is_type.file,
                                   show_col_types = FALSE) |>
  select(of_variable, is_type) |>
  separate_longer_delim(cols = 'of_variable', delim = ',')

undefined_data_type <- data.lvl1.ls$meta |> 
  select(of_variable, is_type) |> 
  unique() |>
  bind_rows(data.lvl1.ls$primary |> 
              select(of_variable, is_type) |> 
              unique()) |>
  #subset for variables that are not 'incomplete'
  semi_join(SoilDRaH_variable_list |>
              filter(project_status != 'incomplete'),
            by = join_by(of_variable)) |>
  #check for of_variable-is_type tuples that are not in the SoilDRaH type definitions
  anti_join(SoilDRaH_type_list,
            by = join_by(of_variable, is_type))


if(nrow(undefined_data_type) == 0){
  print('All of_variables-is_types are in the SoilDRaH type list. No additions required')
}else{
  print(paste0('Please start a issue for new of_variable-is_type [', paste(paste(undefined_data_type$of_variable, undefined_data_type$is_type, sep = '-'), collapse = ','), ']'))
}
```


```{r}
#| label: checkReadFunction_lvl1

datafunction.lvl1.ls <- readAres2001(dataDir = '.',
                                    dataLevel = 'level1')

if(isTRUE(all.equal(datafunction.lvl1.ls, data.lvl1.ls))){
  print('Read function matches code for level 1.')
}else{
  print('There is a mismatch in the level 1 data objects between the code here and the read function.')
}

```

# Level 2 data: HiSOC Database

The structure of level 2 data will change based on the target analysis however level 2 will always be generated from level 1.

```{r}

# Define the variables that we want specificly for HiSOC database, this chould be expanded to other data collections later so we filter here even though it has no effect
HISOC_variables <- c(
  "citation",
  "doi",
  "region", 
  "land_use_type",
  "observation_year",
  'elevation',
  'mean_air_temperature',
  'total_rainfall',
  "soil_organic_carbon",
  "soil_nitrogen",
  "soil_phosphorus",
  "soil_ph",
  "soil_class",
  "stand_age",
  'stand_type'
)

# Pull the study table from the meta data
study.df <- data.lvl1.ls$meta |>
  #look for any of the HiSOC variables
  filter(of_variable %in% HISOC_variables) |>
  #Drop the source infomration. Here we don't care about the source although we can always go back and reconstruct it. 
  select(-source) |> 
  #elevation units are from two sources, remove duplicates
  unique() |> 
  #Spread this wider so that it's human friendly, ending up with one row table
  pivot_wider(names_from = c(of_variable, is_type), names_sep = '::', values_from = with_entry) |>
  #add the study id in prep for merging it with other datasets
  mutate(study_id = 'Ares2001')
  
#The climate across various elevations needs to be gap-filled. Here we create a linear interpolation to gapfill the temperature and rainfall values based on elevation. Details on model fit are in the comments below and fitted from the data orginally provided.
#
climate_var <- c('mean_air_temperature', 'elevation', 'total_rainfall')

#create an elevation identified climate table
elevation.df <- data.lvl1.ls$primary|>
  filter(of_variable %in% climate_var) |>
  # filter(of_variable %in% HISOC_variables,
  #        !(str_detect(of_variable, 'soil') |
  #          str_detect(of_variable, 'stand')))|>
  select(elevation_id, of_variable, with_entry, is_type) |>
  unique() |>
  #convert from character to do the math
  mutate(with_entry = as.numeric(with_entry)) |>
  #pivot things wider to make filling in missing values easier
  pivot_wider(names_from = c(of_variable, is_type), names_sep = '::', values_from = with_entry) |>
  #apply fitted linear interpolation
  mutate(
    #Figure out linear interpolation with
    #lm(formula = `mean_air_temperature::value` ~ `elevation::value`, data = elevation.df)
    #N = 3, Adjusted R-squared:  0.997 , p-value: 0.02449
    `mean_air_temperature::value` = if_else(is.na(`mean_air_temperature::value`), 23.15 - 0.006429 * `elevation::value`, `mean_air_temperature::value`), 
    #N = 3, Adjusted R-squared:  0.9886; p-value: 0.0482
    `total_rainfall::value` = if_else(is.na(`total_rainfall::value`),
                                      19078.83 - 9.97 * `elevation::value`, `total_rainfall::value`)) |>
  #convert everything back to characters for merging in with the rest of the data
  mutate(across(everything(), as.character)) |>
  #add on the study_id
  mutate(study_id = 'Ares2001')

# Pull in the data from the primary table that we did not interpolate above
layer.df <- data.lvl1.ls$primary |>
  #take the entries in HiSOC_variables taht are not in climate_var
  filter(of_variable %in% setdiff(HISOC_variables, climate_var)) |>
  #similar to above, identify based on of_variable and ignore the source
  select(-c(column_name, source)) |>
  unique() |>
  #make things wide for cleaner joins
  pivot_wider(names_from = c(of_variable, is_type), 
              names_sep = '::', values_from = with_entry) |>
  #row identifier is no longer needed
  select(-row_id) |>
  #add in the study id
  mutate(study_id = 'Ares2001')

# join the temp dfs above together
data.lvl2.df <- full_join(study.df,
                          layer.df, 
                          by = join_by(study_id)) |>
  left_join(elevation.df, 
            by = join_by(elevation_id, study_id)) |>
  #Create a new row idea across the entire study
  mutate(row_id = paste0('ID', 1:n())) |>
  #Make this long again so we have the general data structure of
  #...id - of_variable - is_type - with_entry
  pivot_longer(cols = -c(study_id, elevation_id, row_id),
               values_to = 'with_entry',
               names_sep = '::',
               names_to = c('of_variable', 'is_type'),
               values_drop_na = TRUE) 

```


## HiCSC Visuals

Create some basic histograms to check the ranges and tables.

```{r fig.width=7, fig.height=6}

#
plot.df <- data.lvl2.df |>
  #Use regular expressions to get more detailed about if it's a text or numerical value
  mutate(is_type = 
           case_when(
             str_detect(with_entry, pattern = '^\\d+\\.?\\d*$') ~ paste0(is_type, '_numeric'),
             is_type == 'value' ~ paste0(is_type, '_text'),
             TRUE ~ is_type)) |>
  #put the text and numerical values in different columns
  pivot_wider(names_from = 'is_type',
              values_from = 'with_entry') |>
  #cast the numerical values and create informative lables with units
  mutate(value_numeric = as.numeric(value_numeric),
         label = paste0(of_variable, ' (', unit, ')'))

#make a histogram of entries that have numerical values
ggplot(plot.df |>
         filter(is.finite(value_numeric)) )+
  geom_histogram(aes(x=value_numeric), bins = 10) +
  facet_wrap(~label, scales = 'free')

plot.df |>
  #look at the text data now
  filter(!is.na(value_text)) |>
  #count how often it appears
  reframe(count = n(),
    .by = c(of_variable, value_text)) |>
  #put things in order
  arrange(of_variable, count, value_text) |>
  kbl() |>
  kable_paper()
```

# References