---
title: "Data Rescue: Author YYYY"
format:
  html:
    toc: true
    eval: false #default to not evaluating the chunks, override this the template is completed
date: last-modified
date-format: YYYY-MMMM
bibliography:
  - AuthorYYYY_Methods.bib #contains the citations in the methods section
  - AuthorYYYY.bib #contains citation for the data source, generally a single citation of the primary paper
authors: #author scheme from https://quarto.org/docs/journals/authors.html
  - id: ktoddbrown #use the github name as an id
    name:
      given: Katherine
      family: Todd-Brown
    orcid: 0000-0002-3109-8130
    affiliation:
      - ref: uf-ees
    role: #TODO define these roles and add reference
      - transcription: lead #secondary, review
      - standardization: lead #review
      - curation: lead #review

affiliations:
  - id: uf-ees
    name: University of Florida
    department: Environmental Engineering Sciences
    city: Gainesville
    state: Florida
    country: USA
    address: "365 Weil Hall \nP.O. Box 116580"
    postal-code: 32611
    url: https://essie.ufl.edu/ees/
  - id: uf
    name: University of Florida
    city: Gainesville
    state: Florida
    country: USA
    postal-code: 32611
    url: https://www.ufl.edu/
    
timelog:
  - activity: 
      description: add general UF affilation
      who: ktoddbrown #github username
      #when you started and ended, can be just a year or down to the minute
      start: 2026-01-29 T1530
      #start: YYYY-MM-DD THH:MM
      #alternatively duration can be combined with a being/end
      duration: P05M
  - activity: 
      description: revised execute with only provided files
      who: ktoddbrown #github username
      #when you started and ended, can be just a year or down to the minute
      start: 2025-01-22 T1400
      #start: YYYY-MM-DD THH:MM
      #alternatively duration can be combined with a being/end
      duration: P45M
  - activity: 
      description: Revise based on feedback
      who: ktoddbrown #github username
      #when you started and ended, can be just a year or down to the minute
      end: 2025-11-26
      #start: YYYY-MM-DD THH:MM
      #alternatively duration can be combined with a being/end
      duration: P30M
version: 'vs202512' #template version, remove before use
---


```{r}
#| label: setup
#| include: true
#| warning: false
#| message: false
#| echo: false


library(tidyverse) # data processing
library(kableExtra) # pretty tables
library(bibtex) # reading in BibTex files to R

# data files for level 0
methods.file <- "AuthorYYYY_Methods.md"
tableN.file <- 'AuthorYYYY_TableN.csv'
figureM.file <- 'AuthorYYYY_FigureM.csv'

# Bibliography files
primaryCitation.file <- 'AuthorYYYY.bib'
methodsCitation.file <- 'AuthorYYYY_Methods.bib'

# read function; you will need to create this function after you have your data
#... rescue reviewed and before you finish your data harmonization.
readsource.file <- '../../02_DataHarmonization/readAuthorYYYY.R'
if(file.exists(readsource.file)){
  source(readsource.file)
}

#semantic files for level 1
of_variable.file <- '../../semantics/SoilDRaHVocabulary_of_variable.csv'
is_type.file <- '../../semantics/SoilDRaHVocabulary_is_type.csv'

```


# Data Summary

<!---- Fill in the full citation here, generally found on the journal page and link in the issue that you started on GitHub. Though this is at the top of the document, this should be one of the last things you do because a lot of this will be a direct copy-paste into the issue template. --->

> Full citation here, exact format does not matter but it should be complete enough to find the paper. Please include the doi

For the discussion of this data rescue see the Github issue: https://github.com/ktoddbrown/SoilDRaH/issues/#.

<!--- After you read through the paper write a brief summary of what the study
measured and what the data was used for in. This should be <250 words.--->

AuthorYYYY primarily measures ... 
They use this data to look at ...

## Tables and figures

<!--- Write a brief discription of all the tables and figures in this source. This should include full variable names when possible. It is perfectly fine to restate the captions here or copy other text that feels complete. Please keep this to a single sentence for each table/figure.--->

- Table 
  1) Describe the data in the first table.

- Figure 
  1)  Describe the data in the first figure

## Fit for purpose: ProjectName

<!--- Write a brief discription of the project you are processing this data source for. This could be a copy-paste from other documentation. ---->

This data is identified as a data source for PROJECT.
The purpose of this PROJECT is...

<!--- Include a list of the key variables identified for the project. This is used to identify the elements for the paper for the data rescue. An example is below for a basic geo-located soil carbon stock project. --->

  - Location: The geo-location is given in the site description section of the methods. Both latitude and longitude are given but not the datum/projection that was used.
  - Soil carbon stock: Soil carbon stock was calculated from organic carbon fraction and bulk density and is reported in table 3 across two treatments and one control. Coarse fraction was not reported and does not appear to be included.
  
## Files

<!--- Include a brief explaination to all the files in the folder--->

These files are in the AuthorYYYY data rescue.

- [Readme](README_AUTHORYYYY.qmd)
  + This is the primary file that documents the transcriptions and decision made during data rescue.
- [AuthorYYYY.bib](AuthorYYYY.bib)
  + citation for article transcribed
- [AuthorYYYY_Methods.bib](AuthorYYYY_Methods.bib)
  + citations for the methods section of the article
- [AuthorYYYY_Methods.md](AuthorYYYY_Methods.md)
  + methods transcribed from primary article
- [AuthorYYYY_TableN.csv](AuthorYYYY_TableN.csv)
  + table N from primary article with ...
- [temp/](temp/)
    + scratch folder that will not be archived on GitHub, it should include local copies of the data resource (likely a PDF article) and any spreadsheets or other tools you used for the transcriptions.

# Data Rescue working notes 

## Data Rescue Plan

+ Lead
  - [ ] Read the article or data resource and the discription of the target project
  - [ ] Create a fresh fork of the primary repository https://github.com/ktoddbrown/SoilDRaH/tree/main, it might be useful to rename it 'SoilDRaH_AuthorYYYY'
  - [ ] Create a copy of this template folder and rename it to AuthorYYYY
  - [ ] Fill out Data summary section (except for the Files), the Data Rescue Plan (here!), and update the author section of the yml to reflect who is working on the rescue
  - [ ] Create a GitHub issue from the Data Rescue template and get feedback from a reviewer on the project (Dr Todd-Brown or a grad student typically).
  - [ ] Add the reviewer to the author list in the yml
+ Do
  - [ ] Create a spreadsheet with TableN and export it to csv
  - [ ] Extract points from FigureM and export it to csv
  - [ ] Transcribe or copy the method section
  - [ ] Insert references to bib entries
  - [ ] reformat equations and mathematical symbols as [LaTex](https://en.wikibooks.org/wiki/LaTeX/Mathematics)
  - [ ] reformat text using [Markdown](https://www.markdownguide.org/cheat-sheet/)
  - [ ] Create a bib file with the citations from the methods section
  - [ ] Push to your local fork and get a second transcription pushed to your local fork by someone else on the project
+ Measure
  - [ ] cross check with second transcriber for tables and methods, and resolve any differences
  - [ ] compile ReadMe with all chunks up through the Level 0 data rescue section with `eval: true`
  - [ ] push to your local fork and create a pull request to the main branch of the primary repository
  - [ ] address any revisions suggested in your pull request review
  - [ ] celebrate!

## Table N

<!--- Common table modifications include: inserting an extra column to pivot sub-headers, filling in NA to identify blanks, and adding Latex or Markdown to denote formatting.---->

Table N was modified ....

```{r}
#| label: table1Check
#| eval: false #remove when you have the file transcribed


read_csv(file = tableN.file, #read in the specific file
         skip = 1, #skip the first line with the caption
         #read everything as a character, don't worry about formatting yet
         col_types = cols(.default = col_character())) |>
  kable(caption = read_csv(file = tableN.file, #read in that file again but...
                           n_max = 1, #only read the first line
                           col_names = 'caption', #call it the caption
                           #read everything as a character
                           col_types = cols(.default = col_character()),
                           show_col_types = FALSE #do not return messages
                           )$caption #and take it out of the table
  )
```

## Methods

```{bash}
#| label: compareTranscriptions
#| eval: false
# You will want to repeat the line below to cross check each csv and md file in the
#... data rescue. Having no lines flagged by this command will ensure that each character matches exactly.
diff --strip-trailing-cr --suppress-common-lines -y temp/AuthorYYYY_Methods_ABC.md AuthorYYYY_Methods_BCD.md
```

{{< include AuthorYYYY_Methods.md >}}

<!----
## Citation notes from Methods

Below are the citation notes from the methods section.
This generally doesn't read well in the final report so commenting it out here but it is very useful for tracking citations when you are doing the transcriptions.
Some citations are missing, those are noted here and how best guess were generated.
Think of this as a lab notebook documenting how you found each citation.

- [ ] short citation
  + copy from bib
  + manually entered or direct export
---->

# Level 0 Data rescue

This level 0 data read parses the transcribed files with minimal cleaning or transformations.

```{r}
#| label: readLevel0
#This chunk has two purposes. 
#...1) Check the formatting by reading in everything 
#...2) Create a list of everything to process later in level 1

data.lvl0.ls <- list(
  #Read in a list of all the bib files
  citation = list(
    #Citation for the article transcriptions are pulled from
    primary = read.bib(file = primaryCitation.file), 
    #Citations for all referenced articles
    methods = read.bib(file = methodsCitation.file)
  ),
  #Read in the text transcription of the article's methods section
  method = read_lines(file = methods.file),
  #Read in the results as tables or figure transcriptions. This includes
  #...the caption as well as the tables themselves
  data = list(
    Table1 = list(
      #Read the caption as a text string. Captions are the first cell on 
      #...the first row.
      caption = read_csv(file = tableN.file,
                         col_types = cols(.default = col_character()),
                         n_max = 1, col_names = FALSE)$X1[1],
      #Read in all the data, skipping the first row with the caption and read
      #...in the table as character. This element is a tibble (data.frame).
      primary = read_csv(file = tableN.file,
                         col_types = cols(.default = col_character()),
                         skip = 1)
    ) #End 'Table1' element
    #Repeat for all tables and figures...
   ) #end 'data' element
)

```

The code is also in `readsource.file` and returned with the `dataLevel = 'level0'` argument.
Below verifies that the data from the function matches what is coded above.

```{r}
#| label: checkReadFunction_lvl0

if(file.exists(readsource.file)){
  datafunction.lvl0.ls <- readAuthorYYYY(dataDir = '.',
                                         dataLevel = 'level0')
  
  if(isTRUE(all.equal(datafunction.lvl0.ls, data.lvl0.ls))){
    print('Read function matches code here.')
  }else{
    print('There is a mismatch in the data objects between the code here and the read function.')
  }
}else{
  print('The read function does not exist.')
}
```

# Level 1 data - CURATION PROJECT

This data is identified as a data source for the ...
Broad classes of information that we are looking for include:

<!--- Include description of where the data is located in the source.--->
- Location: 
- Soil carbon stock: 
- Soil type: 
- Land use: 

Information from the methods and other sections are pulled into tables with line by line source citations.
This information can be values, units, methods, or other useful notes and is manually extracted from the transcriptions loaded into the level 0 data above.

The final format of the data frame here will include the following 

  + grouping ids
  + `of_variable`
  + `is_type`
  + `with_entry`
  + `from_source`

The `of_variable` being used here are:

 + citation
 
 <!--- include other variables being used, this should match vocabulary in the SoilDRaHVocabulary resources --->

```{r}
#| label: createLvl1
#| eval: false

#### Method-section only data ####
#Create a table for the data in the method-section that is not directly 
#...associated with any table. Often this is study level information.

studyMeta <- tribble(~of_variable, ~is_type, ~with_entry, ~from_source,
                   'region', 'value', 'Honaunau Forest on the southwestern slopes of Mauna Loa, island of Hawaii', paste('Method ln5:', paste(data.lvl0.ls$method[5], collapse = ' ')))

#### Table/Figure N ####
# Process the primary data from Table/Figure M by maintaining data groups and 
#...standardizing the variable name so that we can link this to the study 
#...metadata and data in the other tables when needed. Link in the methods 
#...and other information from the method-section as needed.

Table1Primary <- data.lvl0.ls$data$Table1$primary |>
  #Add a row id preserving row grouping -- not unique
  mutate(row_id = paste0('R', 1:n())) |>
  #Transform the table to long data by row group and column names.
  #...This allows us to then create the cross links between the tables.
  pivot_longer(cols = -row_id,
               names_to = 'column_name', values_to = 'with_entry',
               values_drop_na = TRUE) |> #drop missing values
  #Link the variable names to the column names using a switch statement
  mutate(of_variable = case_when(
           #Soil PH is given as an example below
           column_name == 'SOC' ~ 'soil_organic_carbon')) |>
  #Flag the source of this data as Table 1, this table is merged later with
  #...other data so this keeps track of where the data came from
  mutate(from_source = 'Table 1')

## Create the metadata for table one from the header information and cited methods
Table1Meta <- Table1Primary |>
  select(column_name, of_variable) |>
  unique() |>
  #Grab everything between the parentheses as units and attribute the source as the column names.
  mutate(unit = str_extract(column_name, pattern = '(?<=\\().*(?=\\))'),
         from_source = 'Table 1 column names.') |>
  #If there aren't units then drop the row
  filter(!is.na(unit)) |>
  #Don't keep the column name now that you have the units, cross link via the variable
  select(-column_name) |>
  #Stack into the table the methods for each variable that we can find
  bind_rows(
  tribble(~of_variable, ~method, ~from_source,
            'soil_organic_carbon', paste0(data.lvl0.ls$method[72:73], collapse = ' '), 'Methods ln72-73') ) |>
  #Stack onto the tables the controlled vocabulary used
  bind_rows(
    tribble(~of_variable, ~control_vocabulary, ~from_source,
            'soil_class', 'Histosol:USDA classification for histosol soil type|Andisols:USDA classification for andisol soil type', 'expert informed')
  ) |>
  # Push the dimensions of the variable into a single column. Making this a long table.
  pivot_longer(cols = c(unit, method, control_vocabulary),
               names_to = 'is_type',
               values_drop_na = TRUE,
               values_to = 'with_entry')

#### Create level 1 #####
## Pull everything together into by stacking the meta and primary data tables
data.lvl1.ls <- list(
  meta = bind_rows(studyMeta, Table1Meta),
  primary = bind_rows(Table1Primary)|>
    arrange(row_id, column_name,
            of_variable, is_type, with_entry, from_source)
  )

```

## Check for new variables

`of_variable` in the data files should be defined in the SoilDRaH Vocabulary files.

```{r}
#| label: check_of_variable

#Read in the list of current 'of_variable' entries
SoilDRaH_variable_list <- read_csv(file =of_variable.file,
                                   show_col_types = FALSE)

#compare the variables in the level 1 tables to the SoilDRaH variables
new_variables <- lapply(data.lvl1.ls, function(xx){
  setdiff(unique(xx$of_variable), 
          unique(SoilDRaH_variable_list$of_variable))
}) |>
  unlist()

#Check that there are no new variables
if(length(new_variables) == 0){
  print('All of_variables are in the SoilDRaH variable list. No additions required')
}else{
  cat(paste('Please start a issue for new variables [',
               new_variables, ']', sep = '', collapse = '\n'))
}
```

## Check for is_type

`is_type` in the data files should be defined in the SoilDRaH Vocabulary files.

```{r}
#| label: check_is_type

#read in the current types and what variable they are associated with
SoilDRaH_type_list <- read_csv(file = is_type.file,
                                   show_col_types = FALSE) |>
  #remove the project definition and only keep the variable and type
  select(of_variable, is_type) |>
  #variables are pushed together in the type defined table. Separate the
  #...entries here
  separate_longer_delim(cols = 'of_variable', delim = ',')

#Pull all the of_variable-is_type pairs from the source data
source_pairs <- tibble()

#If there are a large number of entries, consider replacing this for-loop.
#...However this avoids having to install the plyr or other libraries, and 
#...there are generally no more then 3-5 tables
for(dataTable in data.lvl1.ls){
  source_pairs <- dataTable |>
    select(of_variable, is_type) |> 
    unique() |>
    bind_rows(source_pairs)
}

undefined_data_type <- source_pairs |>
  #subset for variables that are not 'incomplete'
  semi_join(SoilDRaH_variable_list |>
              filter(project_status != 'incomplete'),
            by = join_by(of_variable)) |>
  #check for of_variable-is_type tuples that are not in the SoilDRaH type definitions
  anti_join(SoilDRaH_type_list,
            by = join_by(of_variable, is_type))

#Tell the user what you found.
if(nrow(undefined_data_type) == 0){
  print('All of_variables-is_types are in the SoilDRaH type list. No additions required')
}else{
  print(paste0('Please start a issue for new of_variable-is_type [', paste(paste(undefined_data_type$of_variable, undefined_data_type$is_type, sep = '-'), collapse = ','), ']'))
}
```

## Check function match

Check for an exact match with the read function.

```{r}
#| label: checkReadFunction_lvl1

if(file.exists(readsource.file)){
  datafunction.lvl1.ls <- readAuthorYYYY(dataDir = '.',
                                         dataLevel = 'level1')
  
  if(isTRUE(all.equal(datafunction.lvl0.ls, data.lvl0.ls))){
    print('Read function output matches ReadMe code for level 1.')
  }else{
    print('There is a mismatch in the level 1 data objects between the code here and the read function.')
  }
}else{
  print('The read function does not exist.')
}

```


# References

<!---- This space will be filled with the citations when rendered. Leave this blank. ---->

