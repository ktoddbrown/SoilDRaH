---
title: "Data Rescue: CPEAT 2025"
format:
  html:
    toc: true
    eval: false #default to not evaluating the chunks, override this the template is completed
    code-fold: true
date: last-modified
date-format: YYYY-MMMM
bibliography:
  - CPEAT2025_Methods.bib #contains the citations in the methods section
  - CPEAT2025.bib #contains citation for the data source, generally a single citation of the primary paper
nocite: |
  @* #add all citations
authors: #author scheme from https://quarto.org/docs/journals/authors.html
  - id: ktoddbrown #use the github name as an id
    name:
      given: Katherine
      family: Todd-Brown
    orcid: 0000-0002-3109-8130
    affiliation:
      - ref: uf-ees
    role: #TODO define these roles and add reference
      - transcription: lead #secondary, review
      - standardization: lead #review
      - curation: lead #review
  - id: vaasukimarupaka
    name:
      given: Vaasuki
      family: Marupaka
    affiliation:
      - ref: uf-ees
    role:
      - standardization: lead
affiliations:
  - id: uf-ees
    name: University of Florida
    department: Environmental Engineering Sciences
    city: Gainesville
    state: Florida
    country: USA
    address: "365 Weil Hall \nP.O. Box 116580"
    postal-code: 32611
    url: https://essie.ufl.edu/ees/
    
timelog:
  - activity: 
    description: Review, comment, and finalize level 1
    who: ktoddbrown #github username
    #when you started and ended, can be just a year or down to the minute
    start: 2025-11-29T13:20
    end: 2025-11-29T16:00
    #alternatively duration can be combined with a being/end
    #duration: P3H
  - activity: 
    description: Finalize level 0 read function
    who: ktoddbrown #github username
    #when you started and ended, can be just a year or down to the minute
    #start: 2025-11-20T11:00
    end: 2025-11-28T14:30
    #alternatively duration can be combined with a being/end
    duration: P3H
  - activity: 
    description: Merge with prior work
    who: ktoddbrown #github username
    #when you started and ended, can be just a year or down to the minute
    start: 2025-11-27T11:40
    end: T13:45
    #end: YYYY-MM-DD THH:MM
    #alternatively duration can be combined with a being/end
    duration: P2H
---


```{r}
#| label: setup
#| include: true
#| warning: false
#| message: false
#| echo: false
#| eval: true


library(tidyverse) # data processing
library(kableExtra) # pretty tables
library(bibtex) # reading in BibTex files to R

# data files for level 0
methods.file <- "CPEAT2025_Methods.md"

dataDownload.dir <- 'temp/CPEAT'

# Bibliograph files
primaryCitation.file <- 'CPEAT2025.bib'
methodsCitation.file <- 'CPEAT2025_Methods.bib'

# read function
readsource.file <- '../../02_DataHarmonization/readCPEAT2025.R'
source(readsource.file)

#semantic files for level 1
of_variable.file <- '../../02_DataHarmonization/SoilDRaHVocabulary_of_variable.csv'
is_type.file <- '../../02_DataHarmonization/SoilDRaHVocabulary_is_type.csv'

```


# Data Summary

<!---- Fill in the full citation here, generally found on the journal page and link in the issue that you started on GitHub. Though this is at the top of the document, this should be one of the last things you do because a lot of this will be a direct copy-paste into the issue template. --->

> PANGAEA. n.d. “Past Global Changes - Carbon in Peat on EArth Through Time.” https://www.pangaea.de/?q=project:label:PAGES_C-PEAT. (@PCPEAT)

Also see @CPEAT_web


<!--- After you read through the paper write a brief summary of what the study
measured and what the data was used for in. This should be <250 words.--->

CPEAT primarily measures soil organic carbon stocks of peat cores with geolocation and depth.
They use this data to look at global peat stock patterns (@Loisel2021aa).

## Where is the data

This data was downloaded from [https://www.pangaea.de/?q=project:label:PAGES_C-PEAT](https://www.pangaea.de/?q=project:label:PAGES_C-PEAT) using the project label `PAGES_C-PEAT` and the search function of the package `pangaear` to identify 870 unique DOIs.
Some of these DOIs have been superceded and are flagged as 'replaced by'.
All DOIs are listed in the methods BibTex file and can be compared to the `pangaear::pg_search` code in the `checkDOIs` chunk.

There is some one-off data collections for ingest that means we can not ensure that future data contributions will be consistent with what we present here.
This should be a good starting point however for adding new contributions.

```{r}
#| label: makeDownloadURLs


allMethodCitations <- bibtex::read.bib(file = methodsCitation.file)

allURLs.df <- plyr::ldply(allMethodCitations, 
                          .fun = function(xx){
                            return(c(primary = xx$url))
                          },
                          .id = 'bibKey') |>
  filter(bibKey != 'Loisel2021aa') |>
  mutate(downloadData = sprintf('%s?format=textfile', 
                                str_replace(primary, 'doi.org', 'doi.pangaea.de')))
```

```{r}
#| label: checkDOIs
#| eval: false
#| echo: false

### Query the pangaea repository
### 
temp.df <- pangaear::pg_search(query = 'project:label:PAGES_C-PEAT', count = 500) |>
  bind_rows(
    pangaear::pg_search(query = 'project:label:PAGES_C-PEAT', offset = 500, count = 500))

#is there anything new?
setdiff(paste0("https://doi.pangaea.de/",temp.df$doi, "?format=textfile"),
        allURLs.df$downloadData)
#is there anything "lost" from the search
# setdiff(allURLs.df$downloadData,
#         paste0("https://doi.pangaea.de/",temp.df$doi, "?format=textfile"))
```

## Fit for purpose: ProjectName

<!--- Write a brief discription of the project you are processing this data source for. This could be a copy-paste from other documentation. ---->

This data is identified as a data source for PROJECT.
The purpose of this PROJECT is...

<!--- Include a list of the key variables identified for the project. This is used to identify the elements for the paper for the data rescue. An example is below for a basic geo-located soil carbon stock project. --->

- Location: The geo-location is given in the site description section of the methods. Both latitude and longitude are given but not the datum/projection that was used.
- Soil carbon stock: Soil carbon stock was calculated from organic carbon fraction and bulk density and is reported in table 3 across two treatments and one control. Coarse fraction was not reported and does not appear to be included.

## Files

<!--- Include a brief explaination to all the files in the folder--->

These files are in the AuthorYYYY data rescue.

- [Readme](README_AUTHORYYYY.qmd)
+ This is the primary file that documents the transcriptions and decision made during data rescue.
- [AuthorYYYY.bib](AuthorYYYY.bib)
+ citation for article transcribed
- [AuthorYYYY_Methods.bib](AuthorYYYY_Methods.bib)
+ citations for the methods section of the article
- [AuthorYYYY_Methods.md](AuthorYYYY_Methods.md)
+ methods transcribed from primary article
- [AuthorYYYY_TableN.csv](AuthorYYYY_TableN.csv)
+ table N from primary article with ...
- [temp/](temp/)
+ scratch folder that will not be archived on GitHub, it should include local copies of the data resource (likely a PDF article) and any spreadsheets or other tools you used for the transcriptions.

# Level 0 data

Data is available for download (@PCPEAT) and the first thing we do is construct the download files and URLs.

```{r}
#| label: downloadCPEAT
#| code-fold: true

basenames.arr <- allURLs.df$downloadData |> 
  # start with:
  # "https://doi.pangaea.de/10.1594/PANGAEA.889936?format=textfile"
  str_extract(pattern = '(?<=pangaea.de/).*(?=.format)') |> 
  # extract the string between the doi url and the format information:
  # 10.1594/PANGAEA.889936
  str_replace(pattern = '/', replacement = '_') |>
  # replace the forward slash so that it is not treated as a folder
  # 10.1594_PANGAEA.889936
  paste0('.txt')
# add the extension

download.flag <- !file.exists(file.path(dataDownload.dir, basenames.arr))

if(any(download.flag)){
  download.file(url = allURLs.df$downloadData[download.flag], 
                destfile = file.path(dataDownload.dir,
                                     basenames.arr[download.flag]))
}

```

Go through and read in the text of the PANGAEA files.
Then split the meta and primary data.

```{r}
#| label: loadPreLevel0
#| code-fold: true
#| warning: false

basenames.ls <- setNames(object = as.list(basenames.arr),
                         allURLs.df$bibKey)
#str_remove(basenames.arr, '.txt'))

all.text <- plyr::llply(.data = basenames.ls, 
                        .fun = function(basename.str){
                          readr::read_file(file.path(dataDownload.dir, basename.str))
                        })

all.meta <- plyr::ldply(.data = all.text,
                        .fun = function(file.str){
                          #### Parse the first level of the metadata ####
                          #file.str <- all.text$`10.1594_PANGAEA.928061`
                          header.str <- file.str |>
                            str_extract(regex('(?<=/\\* ).*(?=\\*/)', multiline=TRUE, dotall=TRUE))
                          
                          meta.df <- header.str |>
                            str_split_1('(?<=\\n)[^\\t]*\\:') |>
                            as.list() |>
                            setNames(c('meta', str_extract_all(header.str, '(?<=\\n)[^\\t]*\\:') |> 
                                         unlist() |>
                                         str_remove('\\:$'))) |>
                            as_tibble_row()
                          return(meta.df)
                        }, .id = 'bibKey') 

all.primary <- plyr::ldply(.data = all.text, .fun = function(file.str){
  
  #### Parse the primary data file ####
  
  header.str <- file.str |> #all.text[388] |> #
    str_extract(regex('(?<=\\*/\\n)[^\\n]*', multiline=TRUE, dotall=TRUE)) |>
    str_split(patter = '\\t') |>
    unlist()
  
  #"10.1594_PANGAEA.928439" index 388 has an extra 3 at the end of the file randomly, this throws a warning but does not affect how the data is read in
  primary.df <- file.str |> #all.text[388] |> #
    str_extract(regex('(?<=\\*/\\n).*', multiline=TRUE, dotall=TRUE)) |>
    read_tsv(col_types = cols(.default = col_character()),
             skip = 1, col_names = FALSE) |>
    mutate(across(.cols = everything(), .fns = trimws)) |>
    mutate(row_id = paste0('R',1:n())) |>
    pivot_longer(cols = -row_id, 
                 names_to = 'column_index', values_to = 'with_entry') |>
    full_join(tibble(column_index = paste0('X', 1:length(header.str)),
                     column_name = header.str),
              by = join_by(column_index))
  
  return(primary.df)
}, .id = 'bibKey')


```

Continue to parse the header information

```{r}
#| label: processHeader
#| code-fold: true

# study.df <- all.meta |>
#   select(doi, citation = Citation, 
#          replaced_by = `Replaced by`, related_to = `Related to`,
#          further_details = `Further details`,
#          projects = `Project(s)`,
#          change_history = `Change history`,
#          license = License,
#          size = Size, abstract = Abstract,
#          keyword = `Keyword(s)`, status = Status) |>
#   mutate(across(everything(), trimws)) |>
#   mutate(across(c(projects, license, keyword, status), as.factor)) |>
#   mutate(replaced_by_doi = str_extract(replaced_by, '10\\.1594/PANGAEA\\.\\d*') |>
#            str_replace('/', '_'))


#### core data (pre-processing) ####
#### to be broken up into coverage, event, and parameter information
core.df <- all.meta |>
  select(bibKey, coverage = Coverage,
         event = `Event(s)`, parameters = `Parameter(s)`) |>
  separate_wider_delim(cols = parameters, 
                       delim = '\t',
                       names = paste0('X', 0:14),
                       too_few = 'align_start')


#identfy and process the sub-patterns
structures.df <- all.meta |>
  select(Coverage, `Event(s)`, Size, `Parameter(s)`) |>
  mutate(
    Coverage_pattern = 
      str_replace_all(Coverage, '\\-?\\d*\\.?\\d+', '%d'),
    Events_pattern = 
      str_replace(`Event(s)`, '^[^\\*]*\\*', '%s') |>
      str_replace(':[^\\*]*$', ': %s') |>
      str_replace_all(':[^\\*]*\\*', ': %s'),
    Size_pattern = 
      str_replace_all(Size, '\\d+', '%d'))

##Do this --
## - [x] Coverage: Separate variables based on * and \n, then by [name] : [entry], then extract unit from entry. Strip white space
## - [x] Events: Separate variables based on * , then by [name] : [entry], process Comments via ; split, then by [name] : [entry]
## -[x] Parameters: Separate and process the parameter descriptions by str subtraction

### size data ####
### number of data points for each bibKey
size.df <- all.meta |>
  select(bibKey, Size) |>
  mutate(value = str_extract(Size, '\\d+'),
         unit = str_remove(Size, '\\d+'),
         of_variable = 'data_size') |>
  mutate(across(everything(), trimws)) |>
  pivot_longer(cols = c(value, unit), names_to = 'is_type', values_to = 'with_entry') |>
  select(bibKey, of_variable, is_type, with_entry)

#### coverage data ####
#### lat/long and core depth interval
coverage.df <- all.meta |>
  select(bibKey, temp_str = Coverage) |>
  mutate(temp_str = trimws(temp_str)) |>
  separate_longer_delim(temp_str, delim = stringr::regex('\\*|\\n')) |>
  separate_wider_delim(temp_str, delim = ':', names = c('of_variable', 'text')) |>
  mutate(across(everything(), trimws)) |>
  mutate(value = str_remove(text, ' c?m$'),
         unit = trimws(str_extract(text, ' c?m$'))) |>
  select(-text) |>
  pivot_longer(cols = c(value, unit), names_to = 'is_type', values_to = 'with_entry', values_drop_na = TRUE) |>
  mutate(across(everything(), trimws))

#### event data (pre-processing) ####
#### lat/long, location (region), elevation, method and device (coring), core recovery length, core penetration depth, and structured comments (processed next)
events.df <- all.meta |>
  select(bibKey, temp_str = `Event(s)`) |>
  mutate(core_name = str_extract(temp_str, '^.*(?= \\* LATITUDE)')) |>
  mutate(temp_str = str_remove(temp_str, '^.* \\* (?=LATITUDE)')) |>
  separate_wider_delim(temp_str, names = paste0('X', 1:10), 
                       delim = ' * ',
                       too_few = "align_start") |>
  pivot_longer(cols = starts_with('X'),
               names_to = 'drop_names',
               values_to = 'component',
               values_drop_na = TRUE) |>
  mutate(is_type = str_extract(component, '^[^:]+'),
         with_entry = str_remove(component, '^[^:]+: ')) |>
  select(bibKey, is_type, with_entry) |>
  mutate(across(everything(), trimws)) |>
  pivot_wider(names_from = is_type, values_from = with_entry)

### event comment data ####
### coring year, peatland type, basal age, basal age depth, core microtopography, core length, number of dates, carbon rate site, peat properties sample size, bulk density sample volumne, surface age, altitude, site name, and comment
events_comment.df <- events.df |>
  select(bibKey, COMMENT) |>
  #remove refernce to core name in comments
  mutate(COMMENT = str_remove(COMMENT,'^.*e[ta]+ils.*(?=c|Coring year)')) |>
  mutate(COMMENT = str_replace(COMMENT, 'sphagnum, Volume of bulk density samples', 'sphagnum; Volume of bulk density samples')) |>#X3
  mutate(COMMENT = str_replace(COMMENT, 'Stordalen_core2 Dominant peat type', 'Stordalen_core2; Dominant peat type')) |> #X2
  mutate(COMMENT = str_replace(COMMENT, 'KOE.', 'KOE')) |>
  mutate(COMMENT = str_replace(COMMENT, '17 samples:', '17 samples - ')) |>
  mutate(COMMENT = str_replace(COMMENT, 'uncal., basal', 'uncal.; basal')) |>
  mutate(COMMENT = str_replace(COMMENT, 'basal age 1', 'basal age: 1')) |>
  separate_longer_delim(cols = 'COMMENT', delim = ';') |>
  mutate(COMMENT = trimws(COMMENT)) |>
  separate_wider_delim(cols = COMMENT, delim = ':',
                       names = c('of_variable', 'with_entry'), too_few = 'align_end') |>
  filter(!(is.na(with_entry) & is.na(of_variable))) |>
  mutate(of_variable = if_else(is.na(of_variable), 
                               'COMMENT', of_variable)) |>
  mutate(of_variable = paste0('EVENT::COMMENT::',str_to_lower(of_variable)))

### event (clean) data ####
### lat/long, location (region), elevation, method and device (coring), core recovery length, and core penetration depth
### Note structured comments are processed above seperately
events_clean.df <- events.df |>
  select(-COMMENT) |>
  pivot_longer(-bibKey, names_to = 'of_variable', values_to='temp',
               values_drop_na = TRUE) |>
  mutate(value = str_remove(temp, ' c?m$'),
         unit = trimws(str_extract(temp, ' c?m$'))) |>
  select(-temp) |>
  pivot_longer(cols = c(value, unit), names_to = 'is_type', values_to = 'with_entry', values_drop_na = TRUE) |>
  mutate(of_variable = paste0('EVENT::', of_variable))

#### column information ####
#### column data model includes column index, name as `of_variable`, and `is_type` including: description, unit, comment, MethodDevice, geocode, and PI
column_meta.df <- all.meta |>
  select(bibKey, `Parameter(s)`) |>
  separate_wider_delim(cols = `Parameter(s)`, 
                       delim = '\t',
                       names = paste0('X', 0:14),
                       too_few = 'align_start') |>
  select(-X0) |>
  pivot_longer(cols = starts_with('X'),
               names_to = 'column_index',
               values_to = 'column_name',
               values_drop_na = TRUE) |>
  #mutate(column_name_match = str_detect(column_name, '^.+ \\*( GEOCODE \\*)? PI:.+( \\* METHOD/DEVICE:.+)?( \\* COMMENT:.+)?$')) |>
  mutate(comment = str_extract(column_name, '(?<= \\* COMMENT:).+$'),
         MethodDevice = str_extract(
           str_remove(column_name, '( \\* COMMENT:.+)?$'), #trim the end
           '(?<= \\* METHOD/DEVICE:).+$'), #extraction pattern
         PI = str_extract(
           str_remove(column_name, 
                      '( \\* METHOD/DEVICE:.+)?( \\* COMMENT:.+)?$'), #trim
           '(?<= PI: ).+$'), #extract
         geocode = str_extract(column_name, 'GEOCODE'),
         temp = str_remove(column_name, ' \\*( GEOCODE \\*)? PI:.+( \\* METHOD/DEVICE:.+)?( \\* COMMENT:.+)?$')) |>
  mutate(description = str_remove(temp, '[\\[\\(].*$'),
         unit = str_extract(temp, '(?<=\\[).+(?=\\])'),
         of_variable = str_extract(temp, '(?<=\\().+(?=\\))')) |>
  select(!c(column_name, temp)) |>
  mutate(across(everything(), trimws)) |>
  pivot_longer(cols = -c(bibKey, column_index, of_variable),
               names_to = 'is_type',
               values_to = 'with_entry',
               values_drop_na = TRUE)

###Compile study-level data####
###put everything together except the column and layer data
study_meta.df <- all.meta |>
  #trim out the things that don't need heavy processing
  select(bibKey, citation = Citation, 
         replaced_by = `Replaced by`, 
         related_to = `Related to`, 
         further_details = `Further details`,
         projects = `Project(s)`, 
         change_history = `Change history`, 
         license = License, 
         abstract = Abstract, 
         keywords = `Keyword(s)`, 
         status = Status, 
         header_comment = Comment) |>
  pivot_longer(cols = -bibKey, 
               names_to = 'of_variable', values_to = 'with_entry',
               values_drop_na = TRUE) |>
  mutate(is_type = 'value') |>
  #pull in the processed data
  bind_rows(size.df,
            events_clean.df,
            events_comment.df,
            coverage.df) |>
  mutate(across(everything(), trimws))

####Create level 0 data list####
#put it all together
lvl0_data.ls <- list(study = study_meta.df,
                     columns = column_meta.df,
                     primary = all.primary)

# memory management
#rm(list = setdiff(ls(), variable_keep))
```

There was some decision to make here on if we are going to go with a core based list or data type. 
We went with data type (`study`, `column`, `primary`) instead.

```{r readLevel0}
#This chunk has two purposes. 
#...1) Check the formatting by reading in everything 
#...2) Create a list of everything to process later in level 1

data.lvl0.ls <- list(
  #Read in a list of all the bib files
  citation = list(
    #Citation for the article transcriptions are pulled from
    primary = read.bib(file = primaryCitation.file), 
    #Citations for all referenced articles
    methods = read.bib(file = methodsCitation.file)
  ),
  #Read in the text transcription of the article's methods section
  method = read_lines(file = methods.file),
  #Read in the results as tables or figure transcriptions. This includes
  #...the caption as well as the tables themselves
  data = lvl0_data.ls
)

```

```{r}
#| label: readFunction_lvl0
#| eval: true
#| warning: false

datafunction.lvl0.ls <- readCPEAT2025(dataDir = '.',
                                      dataLevel = 'level0')
```

```{r}
#| label: checkLvl0

if(isTRUE(all.equal(datafunction.lvl0.ls, data.lvl0.ls))){
  print('Read function matches code here.')
}else{
  print('There is a mismatch in the data objects between the code here and the read function.')
}

```

# Level 1 data

```{r}

doi_core <- datafunction.lvl0.ls$data$study |>
  #remove 112 dois that have been replaced
  filter(of_variable %in% 'citation') |>
  mutate(doi = str_extract(with_entry, '(?<=https://doi.org/).*$'))

### Remove replaced cores####
### Also check to make sure we have their replacements
replaced_core <- datafunction.lvl0.ls$data$study |>
  #remove 112 dois that have been replaced
  filter(of_variable %in% 'replaced_by') #|>
mutate(replacement_doi = str_extract(with_entry, '(?<=https://doi.org/).*$'))

if(all(replaced_core$replacement_doi %in% doi_core$doi)){
  if(verbose) message('Replacement cores present in dataset')
}else{
  warning('There are missing replacement cores.')
}

# Take the cores out of each of the data frames
study.df <- datafunction.lvl0.ls$data$study |>
  filter(!('replaced_by' %in% of_variable), .by = bibKey)

column.df <- datafunction.lvl0.ls$data$columns |>
  filter(bibKey %in% study.df$bibKey)

primary.df <- datafunction.lvl0.ls$data$primary |>
  filter(bibKey %in% study.df$bibKey)

#### Move the PI info to the study data ####
pi.df <- column.df |>
  filter(is_type == 'PI') |>
  select(bibKey, with_entry) |>
  unique() |>
  # #Confirm none of bibKeys have more then one PI, moving information to study.df
  # filter(n() > 1,
  #        .by = bibKey)
  mutate(of_variable = 'PI',
         is_type = 'value')

#remove PI from the column.df
column_noPI.df <- column.df |>
  filter(is_type != 'PI') |>
  #drop geocode
  filter(is_type != 'geocode') |>
  mutate(across(everything(), trimws))

####Map core to ISCN4 #####
core.df <- study.df |>
  #add the PI to the study.df
  bind_rows(pi.df)

####Create unique variable groups and map to ISCN4 ####
####some of the variables have the same name but different methods, within the 
####same core (!). Go through and add in an index to group the unique variable
####dimensions

unique_column.df <- column_noPI.df |>
  pivot_wider(names_from = is_type, values_from = with_entry) |>
  reframe(doi_list = str_c(bibKey, column_index, sep = '$', collapse = ';'),
          .by = -c(bibKey, column_index))|>
  mutate(variable_id = paste0('Variable_',1:n()) )|>
  rename(method = MethodDevice) |>
  pivot_longer(cols = c('description', 'unit', 'comment', 'method'),
               names_to = 'is_type', 
               values_to = 'with_entry', values_drop_na = TRUE)

####Move variable ID over to primary data####

expanded_primary.df <- primary.df |>
  full_join(unique_column.df |>
              separate_longer_delim(cols = doi_list, delim = ';') |>
              separate_wider_delim(cols = doi_list, delim = '$', 
                                   names = c('bibKey', 'column_index')) |>
              select(bibKey, column_index, variable_id, of_variable) |>
              unique(),
            by = join_by(bibKey, column_index)) |>
  mutate(is_type = 'value')

#Finally create the variable map that matches the ISCN4 data purpose
lvl1_data.ls <- list(
    #Read in a list of all the bib files
    citation = list(
      #Citation for the article transcriptions are pulled from
      primary = bibtex::read.bib(file = primaryCitation.file), 
      #Citations for all referenced articles
      methods = bibtex::read.bib(file = methodsCitation.file)
    ),
    #Read in the text transcription of the primary website
    method = readr::read_lines(file = methods.file),
    data = plyr::llply(list(core = core.df,
                 variable = unique_column.df,
                 primary = expanded_primary.df),
            .fun = function(xx){
              xx |>
                mutate(of_variable_DRaH = case_when(
                  of_variable == 'C' ~ 'carbon_percent',
                  of_variable == 'TC' ~ 'total_carbon_percent',
                  of_variable == 'TIC' ~ 'inorganic_carbon_percent',
                  of_variable == 'TOC' ~ 'organic_carbon_percent',
                  of_variable == 'Corg dens' ~ 'organic_carbon_density',
                  of_variable == 'DBD' ~ 'dry_bulk_density',
                  of_variable == 'Depth bot' ~ 'layer_bottom',
                  of_variable == 'Depth top' ~ 'layer_top',
                  #of_variable == 'Depth sed' & str_detect(comment, 'LOI') ~ 'layer_middle_LOI',
                  of_variable == 'Depth sed' ~ 'layer_middle',
                  of_variable == 'Samp thick' ~ 'layer_thickness',
                  of_variable == 'LOI' ~ 'loss_on_ignition',
                  of_variable == 'OM' ~ 'organic_matter_percent',
                  of_variable == 'OM dens' ~ 'organic_matter_density',
                  of_variable == "Water wm" ~ 'field_water_percent',
                  
                  of_variable == 'citation' ~ 'citation',
                  of_variable == 'related_to' ~ 'related_research',
                  of_variable == 'projects' ~ 'projects',
                  of_variable == 'license' ~ 'data_license',
                  of_variable == 'header_comment' ~ 'core_comment',
                  of_variable == "EVENT::LATITUDE" ~ 'latitude',
                  of_variable == "EVENT::LONGITUDE" ~ 'longitude',
                  of_variable == "EVENT::LOCATION" ~ 'region',
                  of_variable == "EVENT::METHOD/DEVICE" ~ 'sampling_method',
                  of_variable == "EVENT::ELEVATION" ~ 'elevation',
                  of_variable == "EVENT::ELEVATION START" ~ 'elevation_start',
                  of_variable == "EVENT::ELEVATION END" ~ 'elevation_end',
                  of_variable == "EVENT::COMMENT::coring year" ~ 'observation_year',
                  of_variable == "EVENT::COMMENT::type of peatland"  ~ 'peatland_class',
                  of_variable == "EVENT::COMMENT::peatland type"  ~ 'peatland_class',
                  of_variable == 'EVENT::COMMENT::site name' ~ 'site_name',
                  of_variable == 'PI' ~ 'principal_investigator',
                  TRUE ~ NA_character_
                )) |>
                rename(of_variable_CPEAT = of_variable)
            })
)

```

```{r}
#| label: readFunction_lvl1
#| eval: true
#| warning: false
datafunction.lvl1.ls <- readCPEAT2025(dataDir = '.', 
                                      dataLevel = "level1")
```

```{r}
#| label: checkLvl1

if(isTRUE(all.equal(datafunction.lvl1.ls, lvl1_data.ls))){
  print('Read function matches code for level 1.')
}else{
  print('There is a mismatch in the data objects between the code and the read function at level 1.')
}
```

```{r}
#| label: updateVocab

SoilDRaH_isType <- read_csv(is_type.file) |>
  separate_longer_delim(of_variable, delim = ',')

SoilDRaH_ofVarible <-read_csv(of_variable.file)

CPEAT_variables <- datafunction.lvl1.ls$data$core |>
  select(is_type, of_variable_DRaH) |>
  unique() |>
  bind_rows(datafunction.lvl1.ls$data$variable|>
              select(is_type, of_variable_DRaH) |>
              unique()) |>
  bind_rows(datafunction.lvl1.ls$data$primary|>
              select(is_type, of_variable_DRaH) |>
              unique()) |>
  unique()

missing_variables <- CPEAT_variables |>
  rename(of_variable = of_variable_DRaH) |>
  anti_join(SoilDRaH_ofVarible,
            by = join_by(of_variable)) |>
  select(of_variable) |>
  unique()
  
#cat(paste0(missing_variables,', ,draft\n'))

# go through and add missing variables printed above to the file manually

new_is_type <- CPEAT_variables |>
  filter(!is.na(is_type) & !is.na(of_variable_DRaH)) |>
  rename('of_variable' = 'of_variable_DRaH') |>
  full_join(SoilDRaH_isType |>
              select(is_type, project_definition) |>
              unique()) |>
  bind_rows(SoilDRaH_isType) |>
  unique() |>
  arrange(is_type, of_variable) |>
  reframe(of_variable = paste0(of_variable, collapse = ','),
          .by = c(is_type, project_definition))

#so much here. modify directly
#write_csv(new_is_type, is_type.file)
```

```{r}
#| label: checkVocabulary
#| eval: true

SoilDRaH_isType <- read_csv(is_type.file) |>
  separate_longer_delim(of_variable, delim = ',')

SoilDRaH_ofVarible <-read_csv(of_variable.file)

CPEAT_variables <- datafunction.lvl1.ls$data$core |>
  select(is_type, of_variable = of_variable_DRaH) |>
  unique() |>
  bind_rows(datafunction.lvl1.ls$data$variable|>
              select(is_type, of_variable = of_variable_DRaH) |>
              unique()) |>
  bind_rows(datafunction.lvl1.ls$data$primary|>
              select(is_type, of_variable = of_variable_DRaH) |>
              unique()) |>
  unique() |>
  filter(!is.na(of_variable), !is.na(is_type))

if(all(CPEAT_variables$of_variable %in% SoilDRaH_ofVarible$of_variable)){
  message('Success! All variables are in the DRaH')
}else{
  message('Fail! There are variables not in the DRaH, go back and add them.')
}

missingTypes <- CPEAT_variables |>
  anti_join(SoilDRaH_isType,
            by = join_by(is_type, of_variable))

if(nrow(missingTypes) != 0){
  message('Success! All types are in the DRaH')
}else{
  message('Fail! There are types not in the DRaH, go back and add them.')
}
```

# References

<!---- This space will be filled with the citations when rendered. Leave this blank. ---->

