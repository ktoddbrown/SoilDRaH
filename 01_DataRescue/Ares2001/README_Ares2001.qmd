---
title: "Data Rescue: Ares 2001"
format:
  html:
    toc: true
date: last-modified
date-format: YYYY-MMMM
bibliography:
  - Ares2001_Methods.bib
  - Ares2001.bib
authors:
  - id: ktoddbrown
    name:
      given: Katherine
      family: Todd-Brown
    orcid: 0000-0002-3109-8130
    affiliation:
      - ref: uf-ees
    role: 
      - transcription: review
      - standardization: lead #review
      - curation: lead #review
  - id: colbyGreen1520
    name:
      given: Colby
      family: Green
    affiliation:
      - ref: uf-ees
    role:
      - transcription: lead
  - id: SavaScott
    name:
      given: Savannah
      family: Scott
    affiliation:
      - ref: uf-ees
    role: Transcription
      
affiliations:
  - id: uf-ees
    name: University of Florida
    department: Environmental Engineering Sciences
    city: Gainesville
    state: Florida
    country: USA
    address: "365 Weil Hall \nP.O. Box 116580"
    postal-code: 32611
    url: https://essie.ufl.edu/ees/
  - id: uf
    name: University of Florida
    city: Gainesville
    state: Florida
    country: USA
    postal-code: 32611
    url: https://www.ufl.edu/

timelog:
  - activity: 
      description: update to new template
      who: ktoddbrown #github username
      #when you started and ended, can be just a year or down to the minute
      start: 2025-01-22 T1530
      #start: YYYY-MM-DD THH:MM
      #alternatively duration can be combined with a being/end
      duration: P25M
---

```{r}
#| label: setup
#| include: true
#| warning: false
#| message: false
#| echo: false

library(tidyverse) # data processing
library(kableExtra) # pretty tables
library(bibtex) # reading in BibTex files to R

# data files for level 0
methods.file <- 'Ares2001_Methods.md'
table1.file <- 'Ares2001_Table1.csv'
table3.file <- 'Ares2001_Table3.csv'

# Bibliography files
primaryCitation.file <- 'Ares2001.bib'
methodsCitation.file <- 'Ares2001_Methods.bib'

# read function
readsource.file <- '../../02_DataHarmonization/readAres2001.R'
if(file.exists(readsource.file)){
  source(readsource.file)
}

#semantic files for level 1
of_variable.file <- '../../02_DataHarmonization/of_variable.csv'
is_type.file <- '../../02_DataHarmonization/is_type.csv'

```


# Data Summary


> A. Ares and J. H. Fownes. Productivity, resource use, and competitive interactions of fraxinus uhdei in hawaii uplands. Canadian Journal of Forest Research, 31(1):132-142, [https://doi.org/10.1139/x00-156](https://doi.org/10.1139/x00-156). 2001 

For the discussion of this data rescue of @Ares2001 see Github [issue 77](https://github.com/ktoddbrown/SoilDRaH/issues/77).
Discussion on the data harmonization and curation see Github [issue 183](https://github.com/ktoddbrown/SoilDRaH/issues/183).
Note that this is a prototype data source, issues will be non-standard.

@Ares2001 primarily measures tree growth parameters (including litterfall) but also characterized elemental fractions of soil. 
They use this data to look at factors affecting a specific invasive tree growth in Hawaii including nitrogen and moisture gradients.

## Tables and figures

- Table 
  1) characteristics of stands including oc percent < -- oc == organic carbon -->
  2) regression coefficients linking diameter at breast height to biomass and leaf area
  3) climate
  4) elevation vs productivity metrics
  5) fertilizer treatment vs productivity over time
  6) fertilizer vs leaf area index
  7) gradient vs plant efficiency metrics

- Figure 
  1) stand map overlayed on soil
  2) elevation vs productivity metrics
  3) nitrogen correlations
  4) light intercept vs nitrogen and water use efficiency metrics


## Fit for purpose: HiCSC

This data is identified as a data source for the [Hawai'i Soil Organic Carbon Database](https://osf.io/hmtv6/) as part of the HiCSC.
The purpose of this project is to disentangle the effects of land use transitions, soil type, and soil carbon stock in Hawai'i.

- Location: Location region is identified and lat/lon can likely be recovered from figure 1 by regional expert.
- Soil carbon stock: Soil carbon fraction is given but not bulk density nor coarse fraction.
- Soil type: Soil order and history are identified in methods. High/low activity clay status may need inference by regional expert.
- Land use: Land use and history are clearly identified in methods.

# Files

These files are in the @Ares2001 data rescue.

- [Readme](README_Ares2001.qmd)
  + This is the primary file that documents the transcriptions and decision made during data rescue.
- [Ares2001.bib](Ares2001.bib)
  + citation for article transcribed
- [Ares2001_Methods.bib](Ares2001_Methods.bib)
  + citations for the methods section of the article
- [Ares2001_Methods.Rmd](Ares2001_Methods.Rmd)
  + methods transcribed from primary article
- [Ares2001_Table1.csv](Ares2001_Table1.csv)
  + table 1 from primary article with site descriptions
- [Ares2001_Table3.csv](Ares2001_Table3.csv)
  + table 3 from primary article with climate variables

<!--- comment out these working notes but preserve them as work done -KTB

# Working notes (KTB)

## Data Rescue Plan

- [x] describe files in human readable form inside ReadMe
- [x] description with Git issue with link
- [x] add contribution yaml
- [x] create excel sheet with Table 1 and 3
- [x] export excel table to csv
- [x] copy over methods section into markdown 
  - started, PDF doesn't copy but there is an html version which is incomplete and error ridden. Ended up with alot of retyping.
- [x] pull down citations in methods section to bib file
- [x] add in citation links to methods
- [x] cross check with second transcriber for tables and methods
- [ ] submit to independent review
- [ ] archive on SoilDRaH


## Citation notes from Methods

Below are the citation notes from the methods section.
Some citations are missing, those are noted here and how best guess were generated.

- [x] Sato et al. 1973
  + Sato, H.H., Ikeda, W., Paeth, R., Smythe, R., and Takehiro, M. 1973. Soil survey of the island of Hawaii, state of Hawaii. USDA Soil Conservation Service, University of Hawaii Agricultural Experiment Station, U.S. Government Printing Office, Washington D.C.
  + manually entered 
- [x] Harrington and Fownes (1993)
  + Harrington, R.A., and Fownes, J.H. 1993. Allometry and growth of planted versus coppice stands of four fast-growing tropical tree species. For. Ecol. Manage. 56: 315-327.
  + direct export
- [x] Sprugel 1983
  + Sprugel, D.G. 1983. Correcting for bias in log-transformed allometric equations. Ecology, 64: 209-210.
  + direct export
- [x] Avery 1975
  + Avery, T.E. 1975. Natural resources measurements. McGraw-Hill Inc., New York.
  + Manual add as a @book
- [x] Welles and Norman 1991
  + Welles, J.M., and Norman, J.M. 1991. Instrument for indirect measurement of canopy architecture. Agron. J. 83: 818-825.
  + direct export
- [x] Nelson and Sommers ~1972~ 1973
  + Nelson, D.W., and Sommers, L.E. 1972. Determination of total nitrogen in plant material. Agron. J. 65: 109-112.
  + correction to the year, 1973 instead
- [x] Isaac and Johnson 1983
  + Isaac, R.A., and Johnson, W.C. 1983. High speed analysis of agriculture samples using inductively coupled plasma-atomic emission spectroscopy. Spectrochim. Acta, 38B: 277-282.
  + direct export
- [x] Wolf 1974
  + Wolf, B. 1974. Improvements in the azomethine-H-method for the determination of boron. Comm. Soil Sci. Plant Anal. 5: 39-44.
  + direct export
- [x] Vitousek and Sanford 1986
  + Vitousek, PM., and Sanford, R.L. 1986. Nutrient cycling in moist tropical forests. Annu. Rev. Ecol. Syst. 17: 137-167.
  + direct export
  + correct number citation
- [x] Heanes 1984
  + Heanes, D. L. 1984. Determination of total organic C in soils by improved chromic acid digestion and spectrophotometric procedure. Commun. Soil Sci. Plant Anal. 15: 1191-1213.
  + direct export
--->

### Table 1: Stand characteristics

Table 1 was modified to remove the sub-tables and flatted sub-tables in certain columns.

```{r table1}
#| code-fold: true
#| message: false

read_csv(file = table1.file,
                   skip = 1,
                   col_types = cols(.default = col_character())) |>
  kable(caption = read_csv(file = table1.file, 
                                 n_max = 1, col_names = 'caption', 
                                 show_col_types = FALSE)$caption)
```

### Table 3: Climate

```{r table3}
#| code-fold: true
#| message: false

read_csv(file = table3.file,
                   skip = 1,
                   col_types = cols(.default = col_character())) |>
  kable(caption = read_csv(file = table3.file, 
                                 n_max = 1, col_names = 'caption', 
                                 show_col_types = FALSE)$caption)
```

{{< include Ares2001_Methods.md >}}

<!--  # Transcription comparison
what purpose does this serve? this file does not exist in this file group -BW
This is here mostly to remind transcribers to cross check using diff the two transcripts. Let's keep this here but comment it out. -KTB
```{bash eval=FALSE}
diff Ares2001_Methods_CAG.md Ares2001_Methods.md
```
 -->

# Level 0 data read

This level 0 data read parses the transcribed files with minimal cleaning or transformations.

```{r}
#| label: readLevel0
#| code-fold: true
#| code-summary: "This code reads in all lvl0 data."

data.lvl0.ls <- list(
  #Read in a list of all the bib files
  citation = list(
    #Citation for the article transcriptions are pulled from
    primary = read.bib(file = primaryCitation.file), 
    #Citations for all referenced articles
    methods = read.bib(file = methodsCitation.file)
  ),
  #Read in the text transcription of the article's methods section
  method = read_lines(file = methods.file),
  #Read in the results as tables or figure transcriptions. This includes
  #...the caption as well as the tables themselves
  data = list(
    Table1 = list(
      #Read the caption as a text string. Captions are the first cell on 
      #...the first row.
      caption = read_csv(file = table1.file,
                         col_types = cols(.default = col_character()),
                         n_max = 1, col_names = FALSE)$X1[1],
      #Read in all the data, skipping the first row with the caption and read
      #...in the table as character. This element is a tibble (data.frame).
      primary = read_csv(file = table1.file,
                         col_types = cols(.default = col_character()),
                         skip = 1)
    ), 
    #Same format as Table1
    Table3 = list(
      caption = read_csv(file = table3.file,
                         col_types = cols(.default = col_character()),
                         n_max = 1, col_names = FALSE)$X1[1],
      primary = read_csv(file = table3.file,
                         col_types = cols(.default = col_character()),
                         skip = 1)
    )))

```


The code is also in `readAres2001.R` and returned with the `dataLevel = 'level0'` argument.
Below verifies that the data from the function matches what is coded above.

```{r}
#| label: checkReadFunction_lvl0
#| code-fold: true
#| code-summary: "This code checks that the read function matches exactly."

datafunction.lvl0.ls <- readAres2001(dataDir = '.',
                                    dataLevel = 'level0')

if(isTRUE(all.equal(datafunction.lvl0.ls, data.lvl0.ls))){
  print('Read function matches code here.')
}else{
  print('There is a mismatch in the data objects between the code here and the read function.')
}

```

# Level 1 data - Hawaii SOC DB

This data is identified as a data source for the [Hawai'i Soil Organic Carbon Database](https://osf.io/hmtv6/) as part of the HiCSC.

- Location: Location region is identified and lat/lon can likely be recovered from figure 1 by regional expert. Note this was not done in the current data recovery
- Soil carbon stock: Soil carbon fraction is given but not bulk density nor coarse fraction.
- Soil type: Soil order and history are identified in methods. High/low activity clay status may need inference by regional expert.
- Land use: Land use and history are clearly identified in methods.

Information from the methods and other sections are pulled into tables with line by line source citations.
This information can be values, units, methods, or other useful notes and is manually extracted from the transcriptions loaded into the level 0 data above.

The final format of the data frame here will include the following 

  + study
    + `of_variable`
    + `is_type`
    + `with_entry`
    + `from source`
  + primary_meta
    + `column_name` (id column)
    + `of_variable`
    + `is_type`
    + `with_entry`
    + `from source`
  + primary
    + `row_id` (id column)
    + `elevation_id` (id column)
    + `column_name` (id column)
    + `of_variable`
    + `is_type`
    + `with_entry`
    + `from source`

The `of_variable` being used here are:

 + citation
 + diameter_at_breast_height
 + doi
 + elevation
 + height
 + land_use
 + mean_air_temperature
 + mean_max_vapor_pressure_deficit
 + mean_solar_radiation
 + observation_time
 + region
 + soil_class
 + soil_nitrogen
 + soil_organic_carbon
 + soil_ph
 + soil_phosphorus
 + stand_age
 + stand_type
 + stem_density
 + total_rainfall

```{r}
#| lable: constructLevel1

#### Method section only data ####
#Create a table for the data in the methods section that is not directly 
#...associated with any table.
studyMeta <- tribble(~of_variable, ~is_type, ~with_entry, ~from_source,
                   'region', 'value', 'Honaunau Forest on the southwestern slopes of Mauna Loa, island of Hawaii', paste('Method ln5:', paste(data.lvl0.ls$method[5], collapse = ' ')), #no actual lat-long, need to get this translated to geo-location
                   'land_use', 'value', 'Plantation: Tree stands', paste('Method ln14-16:', paste(data.lvl0.ls$method[14:16], collapse = ' ')),
                   'land_use', 'interval', '1959/', paste('Method ln14:', data.lvl0.ls$method[14]),
                   'land_use', 'interval_format', 'YYYY/YYYY', NA,
                   'observation_time', 'value', '1996', paste('Method ln 30;53;58:', paste0(data.lvl0.ls$method[c(30,53,58)], collapse = '... ')),
                   'observation_time', 'time_format', 'YYYY', NA,
                   'citation', 'value', format(data.lvl0.ls$citation$primary), 'journal citation',
                   'doi', 'value', data.lvl0.ls$citation$primary$doi, 'journal citation')


#### Table 1 ####
# Process the primary data from Table 1 by maintaining data groups and standardizing the variable name so that we can link this to the study metadata and data in the other tables when needed.
Table1Primary <- data.lvl0.ls$data$Table1$primary |>
  #Add a row id preserving row grouping -- not unique
  mutate(row_id = paste0('R', 1:n())) |>
  #Transform the table to long data by row group and column names. This allows us to then create the cross links between the tables.
  pivot_longer(cols = -row_id,
               names_to = 'column_name', values_to = 'with_entry',
               values_drop_na = TRUE) |> #drop missing values 
  #Link the variable names to the column names using a switch statement
  mutate(of_variable = case_when(
           column_name == "Stand type" ~ 'stand_type',
           column_name == 'Elevation (m)' ~ 'elevation',
           column_name == 'Soil type' ~ 'soil_class',
           column_name == 'Soil pH' ~ 'soil_ph',
           column_name == 'Soil organic carbon (%)' ~ 'soil_organic_carbon',
           column_name == 'Soil N (%)' ~ 'soil_nitrogen',
           column_name == 'Soil P (mg kg<sup>-1</sup>)' ~ 'soil_phosphorus',
           column_name == 'Stand age (years)' ~ 'stand_age',
           column_name == '*F. uhdei* Stem density (trees / ha)' ~ 'stem_density',
           column_name == '*F. uhdei* Mean DBH (cm)' ~ 'diameter_at_breast_height',
           column_name == '*F. uhdei* Mean height(m)' ~ 'height',
           column_name == '*A. koa* Stem density (trees / ha)' ~ 'stem_density',
           column_name == '*A. koa* Mean DBH (cm)' ~ 'diameter_at_breast_height',
           column_name == "*A. koa* Mean height(m)"~ 'height')) |>
  mutate(is_type = if_else(str_detect(column_name, 'Mean'), 'mean', 'value')) |>
  #Flag the source of this data as Table 1, this table is merged later with other data so this keeps track of where the data came from
  mutate(from_source = 'Table 1')

# Create the metadata for table one from the header information and cited methods
Table1Meta <- Table1Primary |>
  select(column_name, of_variable) |>
  unique() |>
  #Grab everything between the parentheses as units and attribute the source as the column names.
  mutate(unit = str_extract(column_name, pattern = '(?<=\\().*(?=\\))'),
         species = case_when(str_detect(column_name, 'F. uhdei') ~ 'Fraxinus uhdei',
                             str_detect(column_name, 'A. koa') ~ 'Acacia koa',
                             .default = NA_character_),
         from_source = 'Table 1 column names.') |>
  #If there aren't units or a species flag then drop the row
  filter(!is.na(unit) | !is.na(species)) |>
  #Stack into the table the methods for each variable that we can find
  bind_rows(
    tribble(~of_variable, ~method, ~from_source,
            'soil_ph', paste0(data.lvl0.ls$method[72:73], collapse = ' '), 'Methods ln72-73', #pasting in a method from specific rows
            'soil_organic_carbon', paste0(data.lvl0.ls$method[72:73], collapse = ' '), 'Methods ln72-73',
            'soil_nitrogen', paste0(data.lvl0.ls$method[72:73], collapse = ' '), 'Methods ln72-73',
            'soil_phosphorus', paste0(data.lvl0.ls$method[72:73], collapse = ' '), 'Methods ln72-73',
            'soil_class', paste0(data.lvl0.ls$method[10:12], collapse = ' '), 'Methods ln10-12')  |>
      #Stack onto the tables the controlled vocabulary used
      bind_rows(
        tribble(~of_variable, ~control_vocabulary, ~from_source,
                'stand_type', '*F. uhdei*: pure stands of Fraxinus uhdei (Wenzig) Lingelsh|Mixed: mixed stands of *Fraxinus uhdei* (Wenzig) Lingelsh and *Acacia koa* Grey', 'Abstract ln1',
                'soil_class', 'Histosol:USDA classification for histosol soil type|Andisols:USDA classification for andisol soil type', 'expert informed')
      ) |>
      #put the column names back in
      left_join(Table1Primary |>
                  select(column_name, of_variable) |>
                  unique(),
                by = join_by(of_variable)))|>
  # Push the dimensions of the variable into a single column. Making this a long table.
  pivot_longer(cols = c(unit, species, method, control_vocabulary),
               names_to = 'is_type',
               values_drop_na = TRUE,
               values_to = 'with_entry')

#### Table 3 #####
#The following is doing the same process as above but fitted to the data and formatting of Table3
Table3Primary <- data.lvl0.ls$data$Table3$primary |>
  pivot_longer(cols = -variable,
               names_to = 'row_id', values_to = 'with_entry',
               values_drop_na = TRUE) |>
  #mutate(`elevation_id` = str_extract(elevation_id, '\\d{3,} m')) |>
  pivot_wider(names_from = 'variable', values_from = 'with_entry') |>
  mutate(`Elevation (m)` = str_extract(row_id, '\\d{3,}')) |>
  pivot_longer(cols = -row_id, 
               names_to = 'column_name', values_to = 'with_entry') |>
  mutate(of_variable = case_when(
    column_name == "Mean air temperature (degree C)"~ 'mean_air_temperature',
    column_name == "Total rainfall (mm)" ~ 'total_rainfall',
    column_name == "Mean total solar radiation (MJ m<sup>-2</sup> day<sup>-1</sup>)" ~ 'mean_solar_radiation',
    column_name == "Mean maximum vapor pressure deficit (kPa)" ~ 'mean_max_vapor_pressure_deficit',
    column_name == "Elevation (m)" ~ 'elevation',
    TRUE ~ NA_character_)) |>
  mutate(from_source = 'Table 3')

Table3Meta <- Table3Primary |>
  select(column_name, of_variable) |>
  unique() |>
  mutate(unit = str_extract(column_name, pattern = '(?<=\\().*(?=\\))'),
         method = paste('Methods ln72-73:', paste0(data.lvl0.ls$method[18:23], collapse = ' '))) |>
  select(-column_name) |>
  pivot_longer(cols = -of_variable,
               names_to = 'is_type', values_to = 'with_entry') |>
  mutate(from_source = case_when(is_type == 'unit' ~ 'Table 3 column names.',
                            is_type == 'method' ~ 'Methods ln72-73',
                            TRUE ~ NA_character_))

#### Create level 1 #####
# Pull everything together into by stacking the meta and primary data tables
data.lvl1.ls <- list(
  study = studyMeta,
  primary_meta = dplyr::bind_rows(Table1Meta, Table3Meta),
  primary = bind_rows(Table1Primary, Table3Primary)|>
    mutate(elevation_id = with_entry[of_variable == 'elevation'],
           is_type = 'value',
           .by = row_id) |>
    arrange(row_id, elevation_id, column_name,
            of_variable, is_type, with_entry, from_source)
  )

```

## Check for new variables

`of_variable` in the data files should be defined in the SoilDRaH Vocabulary files.

```{r}
#| label: check_of_variable

#Read in the list of current 'of_variable' entries
SoilDRaH_variable_list <- read_csv(file =of_variable.file,
                                   show_col_types = FALSE)

#compare the variables in the level 1 tables to the SoilDRaH variables
new_variables <- lapply(data.lvl1.ls, function(xx){
  setdiff(unique(xx$of_variable), 
          unique(SoilDRaH_variable_list$of_variable))
}) |>
  unlist()

#Check that there are no new variables
if(length(new_variables) == 0){
  print('All of_variables are in the SoilDRaH variable list. No additions required')
}else{
  cat(paste('Please start a issue for new variables [',
               unique(new_variables), ']', sep = '', collapse = '\n'))
}
```

## Check for is_type

`is_type` in the data files should be defined in the SoilDRaH Vocabulary files.

```{r}
#| label: check_is_type

#read in the current types and what variable they are associated with
SoilDRaH_type_list <- read_csv(file = is_type.file,
                                   show_col_types = FALSE) |>
  #remove the project definition and only keep the variable and type
  select(of_variable, is_type) |>
  #variables are pushed together in the type defined table. Separate the
  #...entries here
  separate_longer_delim(cols = 'of_variable', delim = ',')

#Initialize variable for pulling all the of_variable-is_type pairs
source_pairs <- tibble()

#If there are a large number of entries, consider replacing this for-loop.
#...However this avoids having to install the plyr or other libraries, and 
#...there are generally no more then 3-5 tables
for(dataTable in data.lvl1.ls){
  source_pairs <- dataTable |>
    select(of_variable, is_type) |> 
    unique() |>
    bind_rows(source_pairs)
}

undefined_data_type <- source_pairs |>
  #subset for variables that are not 'incomplete'
  semi_join(SoilDRaH_variable_list |>
              filter(project_status != 'incomplete'),
            by = join_by(of_variable)) |>
  #check for of_variable-is_type tuples that are not in the SoilDRaH type definitions
  anti_join(SoilDRaH_type_list,
            by = join_by(of_variable, is_type)) |>
  unique()

#Tell the user what you found.
if(nrow(undefined_data_type) == 0){
  print('All of_variables-is_types are in the SoilDRaH type list. No additions required')
}else{
  print(paste0('Please start a issue for new of_variable-is_type [', paste(paste(undefined_data_type$of_variable, undefined_data_type$is_type, sep = '-'), collapse = ','), ']'))
}
```

## Check function match

Check for an exact match with the read function.

```{r}
#| label: checkReadFunction_lvl1
if(file.exists(readsource.file)){
  datafunction.lvl1.ls <- readAres2001(dataDir = '.',
                                       dataLevel = 'level1')
  
  if(isTRUE(all.equal(datafunction.lvl1.ls, data.lvl1.ls))){
    print('Read function output matches ReadMe code for level 1.')
  }else{
    print('There is a mismatch in the level 1 data objects between the code here and the read function.')
  }
}else{
  print('No read function found. Moving on')
}
```

# Level 2 data: HiSOC Database

The variables of interest for the HiSOC database translate nicely into a set of four tables tables: source, site, layer, and land use. Rearranging the variables above we get:

- source
  + citation
  + doi
- site
  + region
  + elevation
  + observation_time
  + mean_air_temperature
  + mean_max_vapor_pressure_deficit
  + mean_solar_radiation
  + total_rainfall
  + soil_class
- layer
  + soil_organic_carbon
  + soil_ph
  + soil_phosphorus
  + soil_nitrogen
- site_history
  + land_use

This drops the following variables
  - drop variables
    + stand_type
    + stem_density
    + diameter_at_breast_height
    + height
    + 'mean_max_vapor_pressure_deficit'
    + 'mean_solar_radiation'

Note that the climate variables were provided for elevations that did not match the site elevation. 
We used linear interpolation from the provided elevations to the needed elevations.

```{r}
#| label: buildLvl2

# Define the variables that we want specifically for HiSOC database, this could be expanded to other data collections later so we filter here even though it has no effect
HISOC_variables <- list(
  source = c('citation', 'doi'),
  site = c('region', 'observation_time', #temporal-geolocation
            #climate
            'mean_air_temperature', 'total_rainfall',
            #geology/geography
            'elevation', 'soil_class'),
  layer = c('layer_top', 'layer_bottom', #in methods
            'soil_organic_carbon', 'soil_ph', 'soil_phosphorus', 'soil_nitrogen'),
  site_history = c('land_use')
)
  
source.df <- data.lvl1.ls$study |>
  filter(of_variable %in% HISOC_variables$source) |>
  mutate(source_id = 'Ares2001')  |>
  select(source_id, of_variable, with_entry) |>
  #all of the source information is from the same from_source value so this is easy
  pivot_wider(names_from = of_variable, values_from = with_entry)

site_history.df <- data.lvl1.ls$study |>
  filter(of_variable %in% HISOC_variables$site_history) |>
  mutate(from_source = from_source[is_type == 'value']) |>
  mutate(is_type = case_match(is_type, 'value' ~ 'description',
                              .default = is_type)) |>
  mutate(column_name = paste0(of_variable, '::', is_type)) |>
  select(column_name, with_entry) |>
  pivot_wider(names_from = column_name, values_from=with_entry) |>
  mutate(source_id = 'Ares2001',
         land_use_id = 'LU1', #only land use in the study
         land_use = 'Plantation' #from description
         ) |>
  select(source_id, land_use_id, land_use, 
         'land_use::description', 'land_use::interval', 'land_use::interval_format')

layer.df <- data.lvl1.ls$primary |>
  #pull the layer variables out of the primary data
  filter(of_variable %in% HISOC_variables$layer) |>
  #rely on the row id and of variable to ID the elements
  select(-c(column_name, from_source, elevation_id))|>
  bind_rows(#bind the primary data with associated meta data
    #... generally this is the unit or methods
    data.lvl1.ls$primary_meta |>
      filter(of_variable %in% HISOC_variables$layer) |>
      select(of_variable, is_type, with_entry) |>
      reframe(row_id = unique(data.lvl1.ls$primary$row_id),
              .by = everything()) 
  ) |>
  # add back in the site/elevation ids now that we have the methods bound
  left_join(data.lvl1.ls$primary |> select(row_id, elevation_id) |> unique(),
            by = join_by(row_id)) |>
  #only keep things if there is an associated value we want in the row
  filter((any(is_type == 'value')), .by = row_id) |>
  #create new column names with the variable and type in it
  mutate(column_name = paste0(of_variable,'::',is_type)) |>
  #covert things to more resonable named IDs and pull in the header-values
  select(row_id, elevation_id, column_name, with_entry) |>
  #make everything wide for the poor humans
  pivot_wider(names_from = column_name, values_from = with_entry) |>
  #layer information transcribed from methods for soil variables
  mutate(`layer::top` = '0', `layer::bottom` = '15', 
         `layer::unit` = 'cm', layer_id = 'L1')

  
#The climate across various elevations needs to be gap-filled. 
#  Here we create a linear interpolation to gapfill the temperature and rainfall
#  values based on elevation. Details on model fit are in the comments below and
#  fitted from the data originally provided.

climate_var <- c('mean_air_temperature', 'elevation', 'total_rainfall')

#create an elevation identified climate table
elevation.df <- data.lvl1.ls$primary|>
  filter(of_variable %in% climate_var) |>
  select(elevation_id, of_variable, with_entry, is_type) |>
  unique() |>
  #convert from character to do the math
  mutate(with_entry = as.numeric(with_entry)) |>
  #pivot things wider to make filling in missing values easier
  pivot_wider(names_from = c(of_variable, is_type), names_sep = '::', values_from = with_entry) |>
  #apply fitted linear interpolation
  mutate(
    #Figure out linear interpolation with
    #lm(formula = `mean_air_temperature::value` ~ `elevation::value`, data = elevation.df)
    #N = 3, Adjusted R-squared:  0.997 , p-value: 0.02449
    `mean_air_temperature::value` = if_else(is.na(`mean_air_temperature::value`), 23.15 - 0.006429 * `elevation::value`, `mean_air_temperature::value`), 
    #N = 3, Adjusted R-squared:  0.9886; p-value: 0.0482
    `total_rainfall::value` = if_else(is.na(`total_rainfall::value`),
                                      19078.83 - 9.97 * `elevation::value`, `total_rainfall::value`)) |>
  mutate(`mean_air_temperature::method` = 'linear interpolation from provided elevation values',
         `total_rainfall::method` = 'linear interpolation from provided elevation values') |>
  #convert everything back to characters for merging in with the rest of the data
  mutate(across(everything(), as.character)) |>
  #add on the source_id
  mutate(source_id = 'Ares2001')

# Pull in the data from the primary table that we did not interpolate above
site.df <- data.lvl1.ls$primary |>
  #take the entries in HiSOC_variables taht are not in climate_var
  filter(of_variable %in% setdiff(HISOC_variables$site, climate_var))|>
  #similar to above, identify based on of_variable and ignore the source
  select(-c(column_name, from_source)) |>
  unique() |>
  #make things wide for cleaner joins
  pivot_wider(names_from = c(of_variable, is_type), 
              names_sep = '::', values_from = with_entry) |>
  bind_cols( #pull in the variables in the study table like region and obs time
    data.lvl1.ls$study |> 
      filter(of_variable %in% HISOC_variables$site) |>
      select(-from_source) |>
      #make things wide for cleaner joins
      pivot_wider(names_from = c(of_variable, is_type),
                  values_from = c(with_entry),
                  names_sep = '::')
  ) |>
  #add in the study id 
  mutate(source_id = 'Ares2001') |>
  #add in the interpolated elevation variables
  left_join(elevation.df,
            by = join_by(elevation_id, source_id))


# return a list of tables

data.lvl2HiCSC.df <- list(
  source = source.df,
  #make sure the site_ids have a letter in front of them so it is not read in as a numerical when read/write to csv
  site = site.df |>
    mutate(site_id = paste0('S', elevation_id, row_id)) |>
    select(-row_id),
  site_history = site_history.df,
  layer = layer.df |>
    mutate(site_id = paste0('S', elevation_id, row_id)) |> 
    select(-row_id)
  )

```


## Check function match

Check for an exact match with the read function.

```{r}
#| label: checkReadFunction_lvl2
if(file.exists(readsource.file)){
  datafunction.lvl2.ls <- readAres2001(dataDir = '.',
                                       dataLevel = 'level2-HiSOC')
  
  if(isTRUE(all.equal(datafunction.lvl2.ls, data.lvl2HiCSC.df))){
    print('Read function output matches ReadMe code for level 2 - HiSOC.')
  }else{
    print('There is a mismatch in the level level 2 - HiSOC data objects between the code here and the read function.')
  }
}else{
  print('No read function found. Moving on')
}
```


## HiCSC Visuals

Create some basic histograms to check the ranges and tables.

```{r fig.width=7, fig.height=6}

bigLong <- data.lvl2HiCSC.df$source |>
  full_join(data.lvl2HiCSC.df$site_history, 
            by = join_by(source_id)) |>
  full_join(data.lvl2HiCSC.df$site, 
            by = join_by(source_id)) |>
  full_join(data.lvl2HiCSC.df$layer,
            by = join_by(elevation_id, site_id)) |>
  pivot_longer(cols = -ends_with('_id'),
               names_to = 'colume_name',
               values_to = 'with_entry') |>
  separate_wider_delim(cols = colume_name,
                       names = c('of_variable', 'is_type'),
                       delim = '::', too_few = 'align_start') |>
  mutate(is_type = if_else(is.na(is_type), 'value', is_type)) #|>
  # pivot_wider(names_from = is_type, values_from = with_entry)

#
plot.df <- bigLong |>
  #Use regular expressions to get more detailed about if it's a text or numerical value
  mutate(is_type = 
           case_when(
             str_detect(with_entry, pattern = '^\\d+\\.?\\d*$') ~ paste0(is_type, '_numeric'),
             is_type == 'value' ~ paste0(is_type, '_text'),
             TRUE ~ is_type)) |>
  #put the text and numerical values in different columns
  pivot_wider(names_from = 'is_type',
              values_from = 'with_entry') |>
  #cast the numerical values and create informative lables with units
  mutate(value_numeric = as.numeric(value_numeric),
         label = paste0(of_variable, ' (', unit, ')'))

#make a histogram of entries that have numerical values
ggplot(plot.df |>
         filter(is.finite(value_numeric)) )+
  geom_histogram(aes(x=value_numeric), bins = 10) +
  facet_wrap(~label, scales = 'free')

plot.df |>
  #look at the text data now
  filter(!is.na(value_text)) |>
  #count how often it appears
  reframe(count = n(),
    .by = c(of_variable, value_text)) |>
  #put things in order
  arrange(of_variable, count, value_text) |>
  kbl() |>
  kable_paper()
```

# References