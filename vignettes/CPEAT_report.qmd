---
title: "Carbon in Peat on EArth through Time"
author: "Vaasuki Marupaka"
date: "Summer 2024"
knitr:
  opts_chunk: 
    collapse: true
    comment: "#>" 
    R.options:
      knitr.graphics.auto_pdf: true
toc: true
toc-depth: 2
---

The purpose of this document is to summarize the portions of the Carbon in Peat on EArth through Time (C-PEAT) Database that are relevant to data collections in the SoilDRaH project and walk through the data ingestion.
Here you will find links to documentation from the data provider, links to where you can access the data, data model description, data processing workflows, and visuals for the collection relevant variables.

# What is C-PEAT

The version of the Carbon in Peat on EArth through Time (C-PEAT) Database described here is the public facing version of the PANGAEA repository (https://www.pangaea.de).

> **C-PEAT Database**
> Peatlands played a key role in the global carbon cycle during the Holocene and previous interglacials. High-latitude and tropical peatlands have acted as net long-term atmospheric sinks for carbon dioxide (CO2). However, key uncertainties remain regarding many fundamental patterns and processes. C-PEAT aims to address these uncertainties.

Taken from [https://pastglobalchanges.org/science/wg/former/peat-carbon/intro](https://pastglobalchanges.org/science/wg/former/peat-carbon/intro) (Accessed 17-June-2024)

The annotations table draws heavily from the parameters described in the orginal metadata and also from the point of contact for C-PEAT collection of datasets (Dr. Julie Loisel). 

The C-PEAT database on pangaea is presented in a complex nested format with the primary study level and metadata information presented as a graphical database structure while the layer level information or the data table are presented in regular relational database format. We have worked with merging this complex data structure and collapsing it into one single table format. For example, when you call the function $str$ to find out the structure of the object, the data is presented in a nested list format for the original CPEAT database structure. Our goal is to shoestring this data into one long table along with the annotations. 

# Data processing

```{r setup, echo=TRUE, warning=FALSE, message=FALSE}

library(readr) # read in the csv tables
library(tibble) # use tibbles instead of a data frame
library(plyr) # transform a list into a data frame for the bind
library(dplyr) # work with data tables filters/joins/reframes
library(tidyr) # work with data table pivots
library(ggplot2) # make plots
library(stringr) # extract text from descriptions
library(knitr) # make prettier tables
library(kableExtra) # make tables scrollable
library(pangaear) # read in data from p

#locate the read script
readCPEAT_file <- 'readCPEAT.R'

#locate the data directory we will download to
dataDir <- 'CPEAT'
dataAnnotations <- 'annotations_CPEAT2.csv'
```

```{r filenames}
#| echo: FALSE
#| eval: TRUE
#Change this file to run locally for you
dataDir  <- '../temp/CPEAT'

readCPEAT_file <- file.path('../R', readCPEAT_file)
dataAnnotations <- file.path('../data', dataAnnotations)
```

```{r}
source(readCPEAT_file, local = knitr::knit_global())
```

```{r longCPEAT_read}
#| cache: TRUE
#| warning: FALSE
#| message: FALSE

CPEAT.ls <- readCPEAT(dataDir, 
                  annotationFilename = dataAnnotations, 
                  #when first running, switch this to true 
                  verbose = FALSE, 
                  format = 'long',
                  randomSubset = Inf) # 2,640,892 obs for all datasets

#read the same example we take below
singleCPEAT <- pangaear::pg_data(doi = '10.1594/PANGAEA.890203')[[1]]
```

## Orginal Pangaea data structure

Each core in CPEAT has it's own doi-identified data package with it's own entry on the Pangaea repository.
Each data package includes graphical metadata at the front of a tab separated data table.
The graphical metadata is listed between `/*` and `*/`.
Identifiers in this graphical format are followed by `:` with aspects of this item shown inline with a ` * `-separation followed by another `:`-denoted key-vale, and child items are preceded by a tab.

See the example below.

```{r showDataStr}
#| echo: false
#| code-fold: true
#| code-summary: 'Example Pangaea File'
#| comment: ''
examplefile <- readLines(list.files(file.path(dataDir), 
                                    full.names = TRUE)[10])
cat(paste0(examplefile, collapse = '\n'))
```

This can be represented as a graph (without the value entries) as follows:

TODO Fill in the rest of the diagram

```{mermaid}
flowchart LR
  A[Citation]
  B[Related to]
  C[Further details]
  D["Project(s)"]
  E[Coverage] --> EA[LATITUDE] & EB[LONGITUDE] & EC["MINIMUM DEPTH, sediment/rock"] & I["MAXIMUM DEPTH, sediment/rock"]
  J["Events(s)"] --> K[" "] & L[LATITUDE] & M[LONGITUDE] & N[Recovery] & O[Location] & P["METHOD/DEVISE"] & Q[COMMENT]
  Q --> QA["Coring year"] & S["core length"] & T["peatland type"] & U[basal age depth] & V[basal age] & W[basal age] & X[n dates] & Y[core microtopography] & Z[carbon rate site] & AA[peat properties site]
```


# Reading the data
This read function is a bit longer and more complicated then others in this collection.
Here we choose to walk through key parts of the code base and explain our approach.
Broadly we used `pangaear` to find, then download and load the data into R.
For the long format, then walked through the list of individual cores using 
`dlply` and generated a single data table in our long format.
Note that the data annotations for this collection are minimal because much of 
the data traditionally encoded as metadata is encoded in the downloaded data.


```{r}
#| echo: false
CPEAT_code <- read_lines(readCPEAT_file)
```

First we find the data on the Pangaea repository using `pangaear::pg_search`.
Search results are limited to the top 500 so we need to repeat this to get the total 876.
Note that if the number of cores in this collection change we need to revisit 
this function to ensure we still have the correct cleaning algorithms.

```{r}
#| eval: true
#| echo: false
#| class-output: R
#| filename: "ln 34-45: readCPEAT.R"
#| comment: ''
cat(paste0(CPEAT_code[34:45], collapse = '\n'))
```


We then download the data and read it in at the same time using the 
`pangaear::pg_data` function.
According to the documentation we should be able to pass a list of doi's but
this does not appear to work right now -- instead we load each doi separately and then piece them back together as a list
using the `plyr::dlply` function.
We also were unable to stop fresh downloads at each run -- as a result
we recommend that you cache the results when possible instead of rerunning this
read function.
 
```{r}
#| eval: true
#| echo: false
#| filename: "ln 69-89: readCPEAT.R"
#| comment: ''
#| class-output: R
#| code-block-border-left: true
cat(paste0(CPEAT_code[69:89], collapse = '\n'))
```

If the original format is requested we then load in the annotations file and
return the results here.
This results in a list of lists as structured below.

```{r readSingle}
#| cache: true
#| echo: false
str(singleCPEAT)
```

Otherwise we move onto transforming this list of lists into a single long data table by iterating through using `plyr::ldply` after filtering the annotations for those
that apply to the primary data (denoted by `--` in the `with_entry` column).


```{r}
#| eval: true
#| echo: false
#| filename: "ln 108-117: readCPEAT.R"
#| comment: ''
#| class-output: R
#| code-block-border-left: true
cat(paste0(CPEAT_code[108:117], collapse = '\n'))
```

### Processing pg_read lists

To each of `pg_read` lists items we apply the following:

First we pull the headers used in the associated data and apply an ordering to them using a `V`-index pattern.
This was done because not all the headers match those documented in the 
parameter descriptions, likely due to an export truncation at some point.
We are instead relying on ordering to match up the column names with the 
parameter descriptions.

```{r}
#| eval: true
#| echo: false
#| filename: "ln 172-197: readCPEAT.R"
#| comment: ''
#| class-output: R
#| code-block-border-left: true
cat(paste0(CPEAT_code[172:197], collapse = '\n'))
```

Then we add the descriptions from the `parameter` and combin that with
`of_variable` tag
from the annotations table.

```{r}
#| eval: true
#| echo: false
#| filename: "ln 199-220: readCPEAT.R"
#| comment: ''
#| code-output: R
# code-block-border-left: true
cat(paste0(CPEAT_code[199:220], collapse = '\n'))
```

Next the `data` element contains the layer level observations.
The column names have been truncated in some cases, so first we replace them using a column number with a `V` prepend and rely on the ordering to match these up with the column descriptions and other information constructed above.
There is no clear row/observation identifier (some tables contain multiple observations for the same layer), so we add a row number with a `R` prepend here to create a unique ID.
Then the table is joined to prior information and pivoted to create a long format.

```{r}
#| eval: true
#| echo: false
#| filename: "ln 223-254: readCPEAT.R"
#| comment: ''
#| class-output: R
#| code-block-border-left: true
cat(paste0(CPEAT_code[223:254], collapse = '\n'))
```

Finally we construct the secondary information about the study stored in elements which are not `parameters` nor `data` that we have not worked with yet


```{r}
#| eval: true
#| echo: false
#| filename: "ln 347-385: readCPEAT.R"
#| comment: ''
#| class-output: R
#| code-block-border-left: true
cat(paste0(CPEAT_code[347:385], collapse = '\n'))
```


The extrated data frames are then bound the rows together.
And then returned to the `plyr::ldply` function to be appended to other core data sets.

```{r}
#| eval: true
#| echo: false
#| filename: "ln 387-388: readCPEAT.R"
#| comment: ''
#| class-output: R
#| code-block-border-left: true
cat(paste0(CPEAT_code[387:388], collapse = '\n'))
```

### Long format output

We then clean up the output and return the resulting long format.

```{r}
#| eval: true
#| echo: false
#| filename: "ln 393-413: readCPEAT.R"
#| comment: ''
#| class-output: R
#| code-block-border-left: true
cat(paste0(CPEAT_code[393:413], collapse = '\n'))
```

This long format has the following structure.

```{r}
CPEAT.ls$long %>%  # 2,640,892 entries, 7 total columns
  slice_head(n=100) %>%
  knitr::kable() %>%
  kable_paper() %>%
  scroll_box(width = "100%", height = "300px")
```

## `table_name`

Often `table_name` will reflect the file name in a set of inter-related csv files, in this case we use it to refer to what `list` the data was extracted from in the `pangaear::pg_data` return.

```{r}
sort(unique(CPEAT.ls$long$table_name))
```

## `is_type`

The `is_type` entry usually falls into one of the following categories: {`value`, `definition`, `unit`, `method`, `control_vocabulary`}. 
This entry describes the type of information associated with `of_variable` found in `with_entry`. 
Note that we use `value` as a general term for simplicity, regardless of whether the data is numerical or text. 

```{r}
CPEAT.ls$long |>
  reframe(count = n(),
          .by = is_type) |>
  arrange(desc(count))
```

## `of_variable`

The of\_variable column describes the variables linked to the column\_id, such as bulk density, organic carbon, vegetation, other covariates. It's important to maintain the original intent of the data providers when creating these annotations. While the of\_variable entries are highly dependent on the dataset, is\_type should have a more restricted range of entries.

```{r}
CPEAT.ls$long |>
  reframe(count = n(),
          .by = of_variable)
```




## CPEAT Data structures



We currently use XX function in the YY package to process this data into a the metadata portion (list-based data) and associated primary data (table-based data).


We annotate this metadata information a little differently then is typical for the project.

The readCPEAT function performs the task of downloading, processing, and transforming datasets related to the CPEAT (PAGES C-PEAT) project. It interacts with the Pangaea data repository using default functions provided by pangaear package, process their nested structures, and transform them into a more uniform, relational data table format that can be manipulated and analyzed more easily. Hereâ€™s a breakdown of the function, what it's doing, and why it is structured this way:

The function begins by searching the Pangaea repository ($pg\_search$) for datasets labeled with the CPEAT project. It includes metadata about these datasets (e.g., DOIs, citations, parameters and events related information). Each dataset (using function $pg\_data$) is downloaded as a structured list from Pangaea, and the function needs to retain this structure temporarily to parse and process it later.

The parameters list is nested under metadata which includes the metadata; description, method, unit, and any additional information aligned with the data table variables. This metadata is often inconsistently formatted (e.g., mismatch of the the length of columns), so the function cleans and standardizes it by ensuring that parameter descriptions match the data columns. The function creates columns out of data table variables/headers by extracting the id, unit, method information using regular expressions ($str\_extract$).

Firstly, the nested lists are transformed into a single wide table with all the columns. For better handling and data manipulation, the function transforms into a long table format ($pivot\_longer$) along with unique column numbers and row ids grouped by specific doi. The long format helps in data analysis, making it easier to perform operations like filtering, aggregating, and visualizing the data. 

In the final output, the long table consists of the columns: $doi$, $table\_name$, $column\_number$, $row\_number$, $of\_variable$, $is\_type$, $with\_entry$ that resemble the typical annotations template we follow for data harmonization efforts. 

The table names for this database include: primary study information (denoted by . in the long table), metadata, metadata\$events, metadata\$parameters, and data tables. 




The with\_entry column contains either the specific entry associated with the variable and type described or a special notation indicating a reference to the dataset being described. We currently use -- to denote a dataset reference.

# Subseting for soil carbon

There is a lot of data here but not all of it is of interest for all purposes.
In this example we are interested in layer-level soil carbon related measurements and the geolocation.
Note that we are NOT checking any of the quality control flags here which you would want to do if using this for research purposes.

```{r}
CPEAT_soc <- CPEAT.ls$long %>% 
  #identify the variables that we are interested in
  filter(of_variable %in% c(
    "doi",
    "data_doi",
    "download_url",
    "carbon",
    "bulk_density",
    "layer_mid",
    "layer_thickness",
    "latitude",
    "longitude",
    "region",
    "elevation",
    "elevation_end",
    "elevation_start",
    "loss_on_ignition",
    "organic_matter",
    "organic_matter_density",
    "total_carbon",
    "total_organic_carbon",
    "inorganic_carbon",
    "peat_description"),
    #limit these to tables since some of them are duplicates
    table_name %in% c('.', 'metadata$events', 'data')) %>%
  #only take the values, just incase there are any methods or units in the table
  filter(is_type == 'value', is.na(is_type) | is_type != 'description') %>%
  unique()

CPEAT_soc %>%
  slice_head(n=100) %>%
  knitr::kable() %>%
  kable_paper() %>%
  scroll_box(width = "100%", height = "300px")
```

## Make small tables

Often this data is presented in three tables: a site level table with lat/lon and year, a layer level table with the depth and physiochemical information, and a meta table with information on the units and descriptions.
We given an example below of how this might be organized.

```{r}
# site level info
site.df <- CPEAT_soc %>%
  filter(of_variable %in% c(
    #"doi",
    "latitude",
    "longitude",
    "region",
    "elevation",
    "elevation_end",
    "elevation_start")) %>%
  filter(!is.na(with_entry)) %>% 
  unique() %>% 
  pivot_wider(names_from = of_variable,
               values_from = with_entry)

#just renaming the object to maintain uniformity in naming across datasets
layer.df <- CPEAT_soc %>% 
  select(doi_primary = doi, table_name, row_number, of_variable, with_entry) %>%
  filter(!is.na(with_entry)) %>%
  pivot_wider(names_from = of_variable,
    values_from = with_entry,
    values_fn = list(with_entry = ~ paste(., collapse = "; "))) %>% 
  separate(peat_description, into = c("peat_description", "peat_description_ref"), 
           sep = ";", fill = "left") %>% 
  select(-doi) %>% 
  rename(doi = 'doi_primary')  # 98,596 obs of 18 variables

```

## Casting values

Finally make sure that the numbers are actually numbers.

```{r}
# changing the structure of the column variables except doi and other character variables
site.df <- site.df %>% 
  dplyr::mutate(across(c(latitude, longitude, elevation), as.numeric),
                region = factor(region))

layer.df <- layer.df %>%  
  mutate(across(-c(doi, table_name, row_number, data_doi, 
                   download_url, region, peat_description, peat_description_ref), as.numeric),
         peat_description = factor(peat_description)) # 98,596 obs of 18 variables

```

# Factor tables  

```{r}
# count of regions for CPEAT layer level data
site.df %>% 
  group_by(region) %>% 
  tally() %>%
  knitr::kable(caption = 'CPEAT layer-level catagorical regions') %>% 
  kable_paper() %>%
  scroll_box(width = "100%", height = "300px")

layer_site <- site.df %>% 
  mutate(has_lat_long = is.finite(latitude + longitude)) %>% 
  group_by(doi, has_lat_long, region) %>% 
  tally() %>% 
  mutate(location_label = if_else(has_lat_long, "geolocated_layer", "unlocated_layer")) %>% 
  ungroup() # 870 obs of 5 variables

layer_taxa <- layer.df %>% 
  group_by(peat_description) %>% 
  tally() 

layer_taxa %>%
  knitr::kable(caption = 'CPEAT layer-level species taxa categories') %>% 
  kable_paper() %>%
  scroll_box(width = "100%", height = "300px") # 92 categories
```

## Figures and Report

Below are some general figures that might be of interest if you are evaluating this database for use in a study.

# Plotting Layer histograms!

```{r}
CPEAT_soc_plot <- CPEAT_soc %>% 
  dplyr::mutate(across(with_entry, as.numeric))

ggplot(CPEAT_soc_plot %>%
         pivot_longer(cols = where(is.numeric), values_drop_na = TRUE)) +
  geom_histogram(aes(x=value)) +
  facet_wrap(~of_variable, scales='free')
```

# Bulk density vs Organic carbon %

```{r}
ggplot(layer.df) +
  geom_point(aes(x = bulk_density, y = carbon), alpha = 0.1)
```

# Bulk density vs Total organic carbon %

```{r}
ggplot(layer.df) +
  geom_point(aes(x = bulk_density, y = total_organic_carbon), alpha = 0.1)
```

# Organic matter content across different taxa

```{r}
ggplot(layer.df) +
  geom_histogram(aes(x = organic_matter, fill = peat_description))
```

# Geolocations

```{r}
# plotting geolocations with lat and long values 
ggplot(site.df) +
  geom_polygon(data = map_data('world'),
               aes(x = long, y = lat, group = group), 
               fill = NA, color = 'lightgrey') +
  geom_point(aes(x=longitude, y = latitude), alpha = .1, color = 'darkorange') + 
  labs(x = NULL, y = NULL)

# categorizing the geolocations using region factor
ggplot(site.df) +
  geom_polygon(data = map_data('world'),
               aes(x = long, y = lat, group = group), 
               fill = NA, color = 'lightgrey') +
  geom_point(aes(x=longitude, y = latitude,
                 fill = region, color = region), size = 1.0) +
  #theme(legend.position = "bottom")  +
  labs(x = NULL, y = NULL)
```

```{r eval=FALSE}
install.packages("plotly")
library(plotly)

# Create the interactive ggplot 
plot_cpeat <- ggplot(site.df) +
  geom_polygon(data = map_data('world'),
               aes(x = long, y = lat, group = group), 
               fill = NA, color = 'lightgrey') +
  geom_point(aes(x = longitude, y = latitude, text = region), 
             alpha = 0.2, size = 0.4, color = 'darkblue') +
  labs(title = 'Geolocation for 870 CPEAT cores',
       x = NULL, y = NULL) +
  theme(axis.text = element_blank(),
        axis.title = element_blank())

# Convert to plotly for interactivity
ggplotly(plot_cpeat, tooltip = "text")
```

# Appendix

## readCPEAT.R

```{r}
#| file: '../R/readCPEAT.R'
#| code-line-numbers: TRUE
#| echo: TRUE
#| eval: FALSE
```

