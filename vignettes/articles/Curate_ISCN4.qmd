---
title: "Curation of ISCN4"
author: "Kathe Todd-Brown"
format:
  html:
    toc: true
    number-sections: true
    code-fold: true
    code-summary: "Show the code"
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
---

Soil carbon stocks are critical components of the land carbon balance.
In a Earth system model context digital soil maps are often used to benchmark and/or parameterize models that simulate the Earth climate over the next 100 years.
Underlying these digital soil maps are large soil survey databases with geo-located soil organic carbon area-densities, further derived from layer-resolved soil carbon volumentric-density profiles.

We have focused on harmonizing two major US soil surveys, one academic-led peat core collection, and one academic-led data compilation:

  + USDA-FS-FIA Database (site: 6 071, layers: 23 155, database access date: 2023 May 23),
  + USDA-NRCS-NCSS Web of Soil Database (site: 115 611, layers: 465 897, database access date: 2023 January 13),
  + CPEAT (site: 169, layers: 99 000, database access date: 2024)
  + ISCN3 data collection (site: 10 312, layer: 54 190, database date: 2015).
  
We developed meta-data annotations to describe the relevant location, observation time, organic carbon fraction, coarse fraction, and bulk density observations.
These meta-data annotations are then combined with data downloaded from public web-servers and this information used to generate a harmonized data base.
This process is documented here.

A brief word of warning, the size of these databases can be considerable for some computers (<10 GBs).
This code is designed to be run locally on a machine that has the capasity to download and work with this file size.
If your computer is primarily scoped for working with text documents and a few spreadsheets, you may not be able to run this code.
This code uses the `plyr`, `tidyverse`, and `RSQLite` packages with additional formatting support from `knitr`, `kableExtra`, and `maps`.


```{r setup, message=FALSE, warning=FALSE}
#| code-summary: "Setup"

#TODO: cite the packages

library(plyr) #TODO: readCPEAT ensure package is installed and package::function call formats used see #55
library(tidyverse) #data structure manipulation
library(RSQLite) #accessing the NCSS sql library
library(knitr) # make prettier tables
library(kableExtra) #make tables scroll-able
library(maps) #make global maps

RscriptsDir <- '../../R'
annotationsDir <- '../../data'
dataDownloadDir <- '../../temp'

#Note that if these change you should also go down and update the appendix read chunks
databaseReads <- list(FIA = 'readFIA.R',
                      ISCN3 = 'readISCN3.R',
                      NCSS = 'readNCSS.R',
                      CPEAT = 'readCPEAT.R')

annotationFiles <- list(FIA = 'FIA_Annotations.csv',
                        ISCN3 = 'ISCN3Annotations.csv',
                        NCSS = 'NCSS_Annotations.csv',
                        CPEAT = 'annotations_CPEAT.csv')

mapFile <- 'ISCN4_map.csv'

source(file.path(RscriptsDir, databaseReads$ISCN3))
source(file.path(RscriptsDir, databaseReads$FIA))
source(file.path(RscriptsDir, databaseReads$NCSS))
source(file.path(RscriptsDir, databaseReads$CPEAT))

#knitr::opts_chunk$set(collapse = TRUE)
```

```{r setDownloadDir echo=FALSE}
#Set this to any alternative download directory, this will change if you are running this file
dataDownloadDir <- '~/Dropbox (UFL)/Research/Datasets'
```

# Data sources

Map the source table and variables to common ones for common ones with ISCN4.
We also include an action note but right now these are not tied directly to code.

```{r readVariableMapping}

ISCN4map <- read_csv(file = file.path(annotationsDir, mapFile),
                     col_types = cols(.default = col_character()),
                     show_col_types = FALSE)

ISCN4map |>
  knitr::kable() |>
  kable_paper() |>
  scroll_box(width = "100%", height = "300px")
```

```{r recordBaseVars}

#list of primary data objects to carry forward
#...this is for memory management. 
#...All current variatbles should be carried forward.
primary_variables <- c(ls() , 'primary_variables')

```

## ISCN3

The International Soil Carbon Network (vs3) Data base is a collection of single studies and an earlier version of the NCSS database.
In this workflow we use the `readISCN3` function ready in the version of ISCN3 [archived on EDI](https://portal.edirepository.org/nis/mapbrowse?scope=edi&identifier=1160&revision=1).
We then remove the NRCS NCSS database (vs 2015) so that we can later replace it with our updated version (vs 2024 - Web of Science).
Finally we subset the observed variables to focus on soil organic carbon measurements.

```{r readISCN3}
#| code-summary: "Read in ISCN3 using readISCN function"

ISCN_lvl0.ls <- readISCN3(dataDir = file.path(dataDownloadDir, '/ISCN3'),
                     annotationFilename = file.path(annotationsDir, annotationFiles$ISCN3),
                     format = 'long',
                     verbose = FALSE)

```

```{r}
#| code-summary: "Subset ISCN3 lvl 1 product"


ISCN3_column_selection <- ISCN_lvl0.ls$annotation |>
  #pull the annotations for the primary data entries
  right_join(ISCN4map,
             by = join_by(study_id,# table_id,
                          of_variable == source_variable)) |>
  filter(with_entry == '--') |>
  select(ends_with('_id'), of_variable, target_variable, is_type)

ISCN3_lvl1 <- list(
  primary = ISCN_lvl0.ls$long |> 
    filter(!str_detect(dataset_name_sub, 'NRCS')) |> # added this here since the NRCS dataset name is under 
    ## dataset_name_sub variable
    mutate(study_id = 'ISCN3') |>
    dplyr::rename(dataset_id = dataset_name_soc, 
           site_id = site_name,
           profile_id = profile_name,
           layer_id = layer_name
           ) |>
    ##nrow 17 967 513
    filter(!str_detect(dataset_id, 'NRCS')) |> #remove the old NCSS data, using update later in code
    ##nrow  2 151 476
    # use the column selection to subset the database
    right_join(ISCN3_column_selection, 
               relationship = "many-to-many",
               by = join_by(study_id, column_id)) |>
    select(table_id, column_id, study_id, dataset_id, profile_id, layer_id, 
           #remove the soc identifier since we are not pulling calculated SOC
           of_variable, target_variable, is_type, with_entry) |>
    #nrow 1 034 386
    #### Remove duplicate observations from ISCN3 ####
   unique() |> #some entries repeated due to soc_id replication of non SOC variables
    #nrow 973 621
    ### Pull the column id into a non-value with_entry and collapse
    ### This takes care of observations with multiple methods
    mutate(with_entry = if_else(is_type != 'value', 
                                paste(column_id, with_entry, sep = ':'), 
                                with_entry)) |>
    #### Kick out to wide format for QAQC ####
  select(study_id, dataset_id, profile_id, layer_id, #_id are the row identifier
         target_variable, is_type, with_entry) |>
    pivot_wider(names_from = c(target_variable, is_type), 
                names_sep = '::',
                values_from = with_entry,
                values_fn = function(xx)paste(unique(xx), collapse = ';\n')), 
  
  #pull in the meta data that is not now in the primary data
  meta = ISCN_lvl0.ls$annotation |>
    right_join(ISCN4map |>
                 filter(study_id == 'ISCN3'),
               by = join_by(study_id, #table_id,
                            of_variable == source_variable)) |>
    filter(with_entry != '--') |>
    mutate(with_entry = if_else(is_type != 'value', 
                                paste(column_id, with_entry, sep = ':'), 
                                with_entry)) |>
    mutate(column_id = paste0(target_variable, is_type, sep = '::')) |>
    select(study_id, column_id, with_entry, method_class = target_method, action_note)
)

#memory management
primary_variables <- c(primary_variables, 'ISCN_lvl0.ls',  'ISCN3_lvl1')
rm(list = setdiff(ls(), primary_variables)) 
```


## Forest Inventory Analysis Database (USDA-FS)

The Forest Inventory Analysis Database from the USDA-FS focuses primarily on tree survey data however it does contain soil information for two layers from 0-4 inches and 4-8 inches as well as litter information.
This data set is large and must be downloaded from [the FIA Datamart](https://www.fs.usda.gov/research/products/dataandtools/tools/fia-datamart).
We then run the `readFIA` function to combine the datatables into a single long format and down select to focus on soil organic carbon stocks.

```{r}
#| code-fold: false

FIA_lvl0.ls <- readFIA(file.path(dataDownloadDir, 'FS_FIA'), 
                  annotationFilename = file.path(annotationsDir, annotationFiles$FIA), 
                  verbose = FALSE, #when first running, switch this to true 
                  format = 'long')
#The FIA data base object is large, 2.3 GB
```

```{r}

FIA_controlVocab <- FIA_lvl0.ls$annotations |>
  filter(is_type == 'control_vocabulary') |>
  tidyr::separate_longer_delim(with_entry, delim = ';') |>
  filter(str_detect(with_entry, regex('\\|'))) |>
  tidyr::separate_wider_delim(with_entry, delim = '|', 
                              names = c('with_entry', 'keyed_value')) %>%
  mutate(is_type = 'value')

FIA_column_selection <- 

FIA_lvl1 <- list(
  primary =  FIA_lvl0.ls$long |>
    #### pull in the target variable that we want to map ####
    right_join(FIA_lvl0.ls$annotation |>
                 #pull the annotations for the primary data entries
                 right_join(ISCN4map,
                            by = join_by(study_id,
                                         of_variable == source_variable),
                            relationship = "many-to-many") |>
                 filter(with_entry == '--') |>
                 select(ends_with('_id'), of_variable, target_variable, is_type),
               relationship = "many-to-many",
               by = join_by(column_id, table_id)) |>
    #### replace the control vocab with the provided text values ####
    left_join(FIA_controlVocab,
              by = join_by(study_id, table_id, column_id, 
                           of_variable, is_type, with_entry)) |>
    mutate(with_entry = if_else(is.na(keyed_value), with_entry, keyed_value)) |>
    select(-keyed_value) |>
    #### Kick out to wide format for QAQC ####
    select(study_id, 
           profile_id = `CN.ENTIRE_PLOT`,
           layer_id = `CN.ENTIRE_SOILS_LAB`, #starts_with('CN.'), #CN is the row identifier
           target_variable, is_type, with_entry) |>
  unique() |>
  pivot_wider(names_from = c(target_variable, is_type), names_sep = '::',
                values_from = with_entry,
                values_fn = function(xx)paste(xx, collapse = '::')),
  
  meta = FIA_lvl0.ls$annotations |>
    #pull the annotations for the primary data entries
    right_join(ISCN4map,
               by = join_by(study_id,
                            of_variable == source_variable),
               relationship = "many-to-many")  |>
    filter(is_type != 'control_vocabulary') |>
    filter(with_entry != '--') |>
    mutate(with_entry = if_else(is_type != 'value', 
                                paste(column_id, with_entry, sep = ':'), 
                                with_entry)) |>
    #### Construct the column names ####
    mutate(column_id = paste0(target_variable, "::", is_type)) |>
    select(study_id, column_id, 
           with_entry, method_class = target_method, action_note)
    
)

#memory management
primary_variables <- c(primary_variables, 'FIA_lvl0.ls', 'FIA_lvl1')
rm(list = setdiff(ls(), primary_variables)) 
```

## NCSS (USDA-NRCS, Web of Soil version)

The National Cooperative Soil Survey (USDA-NRCS) holds 

```{r warning=FALSE}
#| code-fold: false
#| warning: false
#| code-summary: 'Read in the NCSS data'

NCSS_lvl0.ls <- readNCSS(dataDir = file.path(dataDownloadDir, 'NRCS_NCSS_20230922'),
                    annotationFilename = file.path(annotationsDir, 'NCSS_Annotations.csv'),
                   format = 'long',
                   verbose = FALSE)
```

```{r}
#| code-summary: "Subset NCSS to lvl 1 product"

NCSS_lvl1 <- list(
  primary =  NCSS_lvl0.ls$long |> 
    mutate(study_id = 'NCSS', 
           profile_id = paste0('site_key:', site_key,
                               '-pedon_key:', pedon_key)) |>
    select(study_id, profile_id, layer_id = layer_key, #row ids
           table_id, column_id, #original-variable ids
           of_variable, is_type, with_entry) |> #meta information
    ##nrow 5 694 780
    # use the column selection to subset the database
    inner_join(NCSS_lvl0.ls$annotation |>
                 mutate(study_id = 'NCSS') |>
                 select(study_id, table_id, column_id, of_variable, is_type, with_entry) |>
                 filter(with_entry == '--') |>
                 #pull the annotations for the primary data entries
                 right_join(ISCN4map,
                            by = join_by(study_id,#table_id, column_id #not annotated for NCSS
                                         of_variable == source_variable))  |>
                 select(ends_with('_id'), of_variable, target_variable, is_type), 
               relationship = "many-to-many",
               by = join_by(study_id, column_id, of_variable, is_type)) |>
    select(study_id, column_id, profile_id, layer_id, 
           of_variable, target_variable, is_type, with_entry) |>
    #nrow 3 754 657
    ### Pull the column id into a non-value with_entry and collapse
    ### This takes care of observations with multiple methods
    mutate(with_entry = if_else(is_type != 'value', 
                                paste(column_id, with_entry, sep = ':'), 
                                with_entry)) |>
    #### Kick out to wide format for QAQC ####
  select(study_id, profile_id, layer_id,
         target_variable, is_type, with_entry) |>
    pivot_wider(names_from = c(target_variable, is_type), names_sep = '::',
                values_from = with_entry, values_fn = function(xx)paste(xx, collapse = ';\n')), 
  
  #pull in the meta data that is not now in the primary data
  meta = NCSS_lvl0.ls$annotation |>
    mutate(study_id = 'NCSS') |>
    inner_join(ISCN4map |>
                 filter(study_id == 'NCSS'),
               by = join_by(study_id, #table_id
                            of_variable == source_variable)) |>
    filter(with_entry != '--') |>
    mutate(with_entry = if_else(is_type != 'value', 
                                paste(column_id, with_entry, sep = ':'), 
                                with_entry)) |>
    mutate(column_id = paste0(target_variable, is_type, sep = '::')) |>
    select(study_id, column_id, with_entry, method_class = target_method, action_note)
)

#memory management
primary_variables <- c(primary_variables, 'NCSS_lvl0.ls', 'NCSS_lvl1')
rm(list = setdiff(ls(), primary_variables)) 
```


## C-PEAT: Carbon in Peat on EArth through Time

CPEAT is a collection of peat cores focused on soil organic carbon measurements and layer age estimates.
You can find out more about the collection [here](https://www.julieloisel.com/cpeat) with cores available via [PANGAEA](https://www.pangaea.de/?q=c-peat).

```{r readCPEAT}
#| code-fold: false

CPEAT_lvl0.ls <- readCPEAT(dataDir = file.path(dataDownloadDir, '/CPEAT'),
                     annotationFilename = file.path(annotationsDir, annotationFiles$CPEAT),
                     format = 'long',
                     verbose = TRUE)
```

```{r}
#| code-summary: "Subset CPEAT into lvl 1 product"


# Columns in ISCN4 map to be used in subsetting
#...note that the CPEAT "metadata" is held in the primary data so there is
#...no use of the '--' notation in the annotation file

CPEAT_column_selection <- ISCN4map |>
  filter(study_id == 'CPEAT') |>
  select(study_id, of_variable = source_variable, target_variable)

temp <- CPEAT_lvl0.ls$long |> 
    mutate(study_id = 'CPEAT') |>
    #doesn't seem to be any loi depths in here not sure where they went, leaving
    #...clever code for when they do crop up
    #Looks like doi '10.1594_PANGAEA.927849' is the problem core, pinning for later
    # dplyr::mutate(observation_id = ifelse(any('depth_mid_loi' %in% of_variable),
    #                                 if_else(of_variable %in% c('depth_mid_loi', 'loss_on_ignition'),
    #                                         paste0(observation_id, 'LOI'), observation_id),
    #                                 observation_id), .by = c(doi)) 
    ##nrow 663 401
    # use the column selection to subset the database
    inner_join(CPEAT_column_selection, 
               relationship = "many-to-many",
               by = join_by(study_id, of_variable)) |>
    ##nrow 306 262
    select(doi, observation_id, 
           is_type, with_entry, target_variable) |>
    unique() |>
  #dplyr::summarise(n = dplyr::n(), .by = c(doi, observation_id, target_variable, is_type)) |>
  #dplyr::filter(n > 1L) #Looks like doi '10.1594_PANGAEA.927849' is the problem core, pinning for later
    filter(!str_detect(doi, '10.1594_PANGAEA.927849')) #|>
     # pivot_wider(names_from = c(target_variable, is_type),
     #             names_sep = '::',
     #             values_from = with_entry)

CPEAT_lvl1 <- list(
  primary = temp |>
    filter(is_type == 'value') |>
      pivot_wider(names_from = c(target_variable, is_type),
                  names_sep = '::',
                  values_from = with_entry),
    
  meta = temp |>
    filter(is_type != 'value') |>
    select(target_variable, is_type, with_entry) |>
    unique()
)

# 
# #memory management
 primary_variables <- c(primary_variables, 'CPEAT_lvl0.ls',  'CPEAT_lvl1')
 rm(list = setdiff(ls(), primary_variables)) 
```


# Harmonize sources


Examining the contextual metadata above we need to do the following:

  + country and state
    * FIA: Extract the control vocabulary and substitute it into those entries
    * All: Ensure consistent state and country names (capitalization and convention)
    * CPEAT: 'location' field may require manual assignment
  + latitude and longitude
    * All: Check for upper/lower bounds (latitude -180:180, longitude -90:90)
    * All: populate datum for each value or label as unknown
  + layer depth
    * FIA: populate upper and lower bounds from control vocabulary
    * All: check zero location
    * FIA: convert to units to cm
    * CPEAT: convert from mid + thickness to upper/lower bound
  + observation year
    * ISCN3: convert from days since origin (unless under 2100)
    * CPEAT: create max observation year from earliest related to citation
  + bulk density
    * ISCN3: remove known fine earth bulk density
    * All: convert units to g cm-3
    * All: move over method notes to primary data
    * CPEAT: correct unit notation
  + organic carbon, inorganic carbon, and coarse fraction
    * All: convert units to mass-percent
    * All: move over method notes to primary data
    * CPEAT: Check for backing out organic carbon from density 

```{r makeISCN4primary}

ISCN4_primary <- ISCN3_lvl1$primary |>
  bind_rows(
  FIA_lvl1$primary |>
    mutate(`depth_bound::method` = `upper_depth_bound::value`) |>
    select(-c(`upper_depth_bound::value`, 
              `lower_depth_bound::value`)) |>
    mutate(`latitude::method` = 'NAD83') #This applies for non Pacific Island locations, Pacific Island locations are WSG84, no current PI locations - 20240625 KTB
  ) |>
  bind_rows(NCSS_lvl1$primary) |>
  # country/state
  rename(state = `state::value`, country = `country::value`) |>
  mutate(country = if_else(country == 'Unknown', 
                           NA_character_, country),
         state = if_else(state == 'Unknown', 
                         NA_character_, state)) |>
  # if there is a state defined, then the country is in the US
  mutate(country = if_else(is.na(country) & !is.na(state), 
                           'United States', country)) |>
  # latitude/longitude/datum
  rename(latitude = `latitude::value`, 
         longitude = `longitude::value`, 
         datum = `latitude::method`) |>
  mutate(datum = str_remove(datum, pattern = regex('.*\\:')) )|>
  mutate(across(c(latitude, longitude), as.numeric)) |>
  # layer depth
  rename(upper_depth_bound = `upper_depth_bound::value`, 
         lower_depth_bound = `lower_depth_bound::value`) |>
  mutate(across(c(upper_depth_bound, lower_depth_bound), 
                as.numeric)) |>
  mutate(upper_depth_bound = case_when(
    study_id == 'FIA' & str_detect(`depth_bound::method`, '0-4 inch') ~ 0,
    study_id == 'FIA' & str_detect(`depth_bound::method`, '4-8 inch') ~ 4*2.54,
    TRUE ~ upper_depth_bound),
    lower_depth_bound = case_when(
      study_id == 'FIA' & str_detect(`depth_bound::method`, '0-4 inch') ~ 4*2.54,
      study_id == 'FIA' & str_detect(`depth_bound::method`, '4-8 inch') ~ 8*2.54,
      TRUE ~ lower_depth_bound)) |>
  # observation year
  rename(observation_year = `observation_year::value`) |>
  mutate(across(c(observation_year), as.numeric)) |>
  mutate(observation_year = if_else(observation_year < 2300, #does not work for years before 1906
                                    observation_year,
                                    #this is an excel days since origin
                                    lubridate::as_date(observation_year,
                                                       origin = ymd('1900-01-01')) |>
                                      year())) |>
  # bulk density
  dplyr::rename(bulk_density_fine = `bulk_density_fine::value`, 
         bulk_density_whole = `bulk_density_whole::value`, 
         bulk_density_unknown = `bulk_density_unknown::value`) |>
  mutate(across(.cols = c(bulk_density_fine,
                                     bulk_density_whole,
                                     bulk_density_unknown),
                .fns = ~  ifelse(is.na(.x),  NA_real_,
                                  str_split_1(
                                    string = as.character(.x), 
                                      pattern = regex(';\\n') ) |>
                  as.numeric() |>
                  mean())), .by = -c(bulk_density_fine,
                                     bulk_density_whole,
                                     bulk_density_unknown)) |>
  mutate(`bulk_density_fine::method` =
           if_else(is.na(bulk_density_fine), 
                                               NA, `bulk_density_fine::method`),
         `bulk_density_whole::method` = if_else(is.na(bulk_density_whole), 
                                               NA, `bulk_density_whole::method`),
         `bulk_density_unknown::method` = if_else(is.na(bulk_density_unknown), 
                                               NA, `bulk_density_unknown::method`)) |>
  mutate(across(c(bulk_density_fine, bulk_density_whole, bulk_density_unknown),
                .fns = function(xx){
                  if_else(xx < 0 | xx > 4, NA, xx)
                })) |>
  # mass fraction values
  rename(total_carbon = `total_carbon::value`,
         organic_carbon = `organic_carbon::value`,
         inorganic_carbon = `inorganic_carbon::value`,
         loss_on_ignition = `loss_on_ignition::value`,
         coarse_fraction = `coarse_fraction::value`) |>  
  mutate(across(c(total_carbon, organic_carbon, inorganic_carbon, 
                loss_on_ignition, coarse_fraction), 
                .fns = ~  ifelse(is.na(.x),  NA_real_,
                                  str_split_1(
                                    string = as.character(.x), 
                                      pattern = regex(';\\n') ) |>
                  as.numeric() |>
                  mean())), .by = -c(total_carbon, 
                                     organic_carbon, 
                                     inorganic_carbon, 
                loss_on_ignition, coarse_fraction)) |>
  mutate(`coarse_fraction::method` = if_else(is.na(coarse_fraction), 
                                             NA, `coarse_fraction::method`),
         `organic_carbon::method` = if_else(is.na(organic_carbon), 
                                            NA, `organic_carbon::method`),
#          `inorganic_carbon::method` = if_else(is.na(inorganic_carbon), 
#                                            NA, `inorganic_carbon::method`),
          `loss_on_ignition::method` = if_else(is.na(loss_on_ignition), 
                                            NA, `loss_on_ignition::method`),
         `total_carbon::method` = if_else(is.na(total_carbon), 
                                          NA, `total_carbon::method`)) |> 
  mutate(across(c(total_carbon, organic_carbon, 
                  inorganic_carbon, loss_on_ignition, coarse_fraction), .fns = function(xx){
    if_else(xx < 0 | xx > 100, NA, xx)
  })) |>
  select(study_id, dataset_id, profile_id, layer_id, 
         country, state,
         latitude, longitude, datum,
         upper_depth_bound, lower_depth_bound, `depth_bound::method`,
         starts_with('observation_year'),
         starts_with('bulk_density'),
         starts_with('total_carbon'),
         starts_with('organic_carbon'),
         starts_with('inorganic_carbon'),
         starts_with('loss_on_ignition'),
         starts_with('coarse_fraction')) |>
  #factor columns <- move to full table later
  mutate(across(study_id, as.factor)) |>
  mutate(across(c(country, state, datum, ends_with('method')), as.factor))

#TODO: Clean this up
ISCN4_meta <- ISCN3_lvl1$meta |>
  bind_rows(FIA_lvl1$meta)

```

## Histograms

```{r fig.width = 8, fig.height = 16}
plot.df <- ISCN4_primary |>
  pivot_longer(cols = where(is.numeric), names_to = 'variable', values_drop_na = TRUE)

ggplot(plot.df) + 
  geom_histogram(aes(x=value), bins = 30) +
  facet_wrap(~study_id + variable, scales = 'free', ncol = 4)
```

## Location over time

```{r fig.width=8, fig.height=10}
ggplot(ISCN4_primary %>%
         select(study_id, dataset_id, profile_id,
                longitude, latitude, observation_year) %>%
         filter(is.finite(longitude + latitude)) %>%
         unique() %>%
         mutate(decade = floor(observation_year/10) * 10)) +
  geom_polygon(colour="grey", fill="white",
               aes(x=long, y=lat, group=group),
               data = map_data("world")) +
  geom_point(aes(x=longitude, y = latitude), alpha = .1) +
  facet_wrap(~decade + study_id, ncol = 4) +
  theme_bw() +
  theme(axis.text = element_blank(),
        axis.title = element_blank())
```

## Depth observations

```{r}
commonBounds <- c(ISCN4_primary$upper_depth_bound, ISCN4_primary$lower_depth_bound) |>
  unique() |>
  sort()

depth_summary <- ISCN4_primary |>
  select(study_id, dataset_id, profile_id,
         upper_depth_bound, lower_depth_bound, organic_carbon) |>
  filter(is.finite(upper_depth_bound + lower_depth_bound)) |>
  unique() |>
  #slice_head(n=200) |>
  reframe(depth = commonBounds[commonBounds >= upper_depth_bound &
                                              commonBounds < lower_depth_bound],
          .by = everything()) |>
  reframe(count = n(),
          organic_carbon_5 = quantile(organic_carbon, probs = 0.05, na.rm =TRUE),
          organic_carbon_25 = quantile(organic_carbon, probs = 0.3, na.rm =TRUE),
          organic_carbon_50 = quantile(organic_carbon, probs = 0.5, na.rm =TRUE),
          organic_carbon_75 = quantile(organic_carbon, probs = 0.75, na.rm =TRUE),
          organic_carbon_95 = quantile(organic_carbon, probs = 0.95, na.rm =TRUE),
          .by = c(depth, study_id))
```

```{r}
ggplot(depth_summary |>
         filter(count > 30)) +
  geom_line(aes(y=count, x = -depth)) +
  coord_flip() +
  facet_wrap(~study_id, scales = 'free')

ggplot(depth_summary |>
         filter(count > 30, depth < 100)) +
  geom_line(aes(y=count, x = -depth)) +
  coord_flip()  +
  facet_wrap(~study_id, scales = 'free_x')
```

```{r}
ggplot(depth_summary |>
         filter(count > 1e3)) +
  geom_ribbon(aes(ymin = organic_carbon_25, 
                  ymax = organic_carbon_75, x = -depth), fill = 'grey') +
  geom_line(aes(y=organic_carbon_5, x = -depth), linetype = 2) +
  geom_line(aes(y=organic_carbon_50, x = -depth)) +
  geom_line(aes(y=organic_carbon_95, x = -depth), linetype = 2) +
  coord_flip() +
  facet_wrap(~study_id, scales = 'free_x')
```

# Appendix

## Read functions

```{r file=file.path(RscriptsDir, databaseReads$ISCN3)}
#| code-summary: "readISCN3"
#| code-fold: true
```

```{r file=file.path(RscriptsDir, databaseReads$FIA)}
#| code-summary: "readFIA"
#| code-fold: true
```

```{r file=file.path(RscriptsDir, databaseReads$NCSS)}
#| code-summary: "readNCSS"
#| code-fold: true
```

